{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32505dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num docs: 32033\n",
      "vocab size: 27\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The most atomic way to train and inference a GPT with DeepSeek Sparse Attention and RoPE.\n",
    "This file is the complete algorithm with sparse attention and rotary embeddings.\n",
    "Everything else is just efficiency.\n",
    "\n",
    "Extensions to @karpathy's microgpt:\n",
    "- DeepSeek Sparse Attention: attend to local window + top-k global tokens\n",
    "- RoPE: rotary position embeddings instead of learned positional embeddings\n",
    "\"\"\"\n",
    "\n",
    "import os       # os.path.exists\n",
    "import math     # math.log, math.exp, math.sin, math.cos\n",
    "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
    "random.seed(42) # Let there be order among chaos\n",
    "\n",
    "# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)\n",
    "if not os.path.exists('input.txt'):\n",
    "    import urllib.request\n",
    "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
    "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
    "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents\n",
    "random.shuffle(docs)\n",
    "print(f\"num docs: {len(docs)}\")\n",
    "\n",
    "# Let there be a Tokenizer to translate strings to discrete symbols and back\n",
    "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
    "BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\n",
    "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "# Let there be Autograd, to recursively apply the chain rule through a computation graph\n",
    "class Value:\n",
    "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
    "\n",
    "    def __init__(self, data, children=(), local_grads=()):\n",
    "        self.data = data                # scalar value of this node calculated during forward pass\n",
    "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
    "        self._children = children       # children of this node in the computation graph\n",
    "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data + other.data, (self, other), (1, 1))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
    "\n",
    "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
    "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
    "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
    "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
    "    def __neg__(self): return self * -1\n",
    "    def __radd__(self, other): return self + other\n",
    "    def __sub__(self, other): return self + (-other)\n",
    "    def __rsub__(self, other): return other + (-self)\n",
    "    def __rmul__(self, other): return self * other\n",
    "    def __truediv__(self, other): return self * other**-1\n",
    "    def __rtruediv__(self, other): return other * self**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            for child, local_grad in zip(v._children, v._local_grads):\n",
    "                child.grad += local_grad * v.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84773c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params: 3936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the parameters, to store the knowledge of the model.\n",
    "n_embd = 16         # embedding dimension\n",
    "n_head = 4          # number of attention heads\n",
    "n_layer = 1         # number of layers\n",
    "block_size = 16     # maximum sequence length\n",
    "head_dim = n_embd // n_head # dimension of each head\n",
    "local_window = 4    # local attention window size (attend to nearest tokens)\n",
    "top_k_global = 2    # number of top-k global tokens to attend to (sparse attention)\n",
    "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
    "state_dict = {'wte': matrix(vocab_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
    "# Note: RoPE replaces learned positional embeddings, so no 'wpe' here\n",
    "for i in range(n_layer):\n",
    "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
    "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
    "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
    "print(f\"num params: {len(params)}\")\n",
    "\n",
    "# Let there be RoPE, the blessed rotary position embeddings that encode position through rotation\n",
    "def apply_rope(vec, pos_id):\n",
    "    \"\"\"Apply Rotary Position Embeddings to a vector by rotating pairs of dimensions.\"\"\"\n",
    "    result = []\n",
    "    for i in range(0, len(vec), 2):\n",
    "        # Each pair of dimensions rotates by an angle determined by position and dimension index\n",
    "        theta = pos_id / (10000 ** (i / len(vec)))\n",
    "        cos_theta = Value(math.cos(theta))\n",
    "        sin_theta = Value(math.sin(theta))\n",
    "        # Rotation matrix: [cos -sin] [x0]\n",
    "        #                  [sin  cos] [x1]\n",
    "        x0, x1 = vec[i], vec[i+1]\n",
    "        result.append(cos_theta * x0 - sin_theta * x1)\n",
    "        result.append(sin_theta * x0 + cos_theta * x1)\n",
    "    return result\n",
    "\n",
    "# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\n",
    "# Follow GPT-2, blessed among the GPTs, with extensions: RoPE for position, DSA for sparse attention\n",
    "def linear(x, w):\n",
    "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
    "\n",
    "def softmax(logits):\n",
    "    max_val = max(val.data for val in logits)\n",
    "    exps = [(val - max_val).exp() for val in logits]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "def rmsnorm(x):\n",
    "    ms = sum(xi * xi for xi in x) / len(x)\n",
    "    scale = (ms + 1e-5) ** -0.5\n",
    "    return [xi * scale for xi in x]\n",
    "\n",
    "def sparse_attention_mask(current_pos, seq_len):\n",
    "    \"\"\"\n",
    "    DeepSeek Sparse Attention: determine which positions to attend to.\n",
    "    Returns a list of positions to attend to (local window + will select top-k global later).\n",
    "    \"\"\"\n",
    "    # Local window: attend to nearby tokens\n",
    "    local_positions = [p for p in range(max(0, current_pos - local_window + 1), current_pos + 1)]\n",
    "    # Global candidates: all other positions before local window\n",
    "    global_candidates = [p for p in range(current_pos + 1) if p not in local_positions]\n",
    "    return local_positions, global_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19432ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gpt(token_id, pos_id, keys, values):\n",
    "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
    "    x = tok_emb # no positional embedding added here; RoPE applies directly to Q and K\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    for li in range(n_layer):\n",
    "        # 1) Multi-head sparse attention block with RoPE\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
    "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
    "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
    "        keys[li].append(k)\n",
    "        values[li].append(v)\n",
    "        seq_len = len(keys[li])\n",
    "        \n",
    "        x_attn = []\n",
    "        for h in range(n_head):\n",
    "            hs = h * head_dim\n",
    "            # Extract head-specific query\n",
    "            q_h = q[hs:hs+head_dim]\n",
    "            # Apply RoPE to query\n",
    "            q_h = apply_rope(q_h, pos_id)\n",
    "            \n",
    "            # DeepSeek Sparse Attention: determine which positions to attend to\n",
    "            local_positions, global_candidates = sparse_attention_mask(pos_id, seq_len)\n",
    "            \n",
    "            # Compute attention scores for local positions (always attend)\n",
    "            local_scores = []\n",
    "            for t in local_positions:\n",
    "                k_h = [keys[li][t][hs+j] for j in range(head_dim)]\n",
    "                k_h = apply_rope(k_h, t) # Apply RoPE to key\n",
    "                score = sum(q_h[j] * k_h[j] for j in range(head_dim)) / head_dim**0.5\n",
    "                local_scores.append((t, score))\n",
    "            \n",
    "            # Compute attention scores for global candidates (sparse selection)\n",
    "            global_scores = []\n",
    "            for t in global_candidates:\n",
    "                k_h = [keys[li][t][hs+j] for j in range(head_dim)]\n",
    "                k_h = apply_rope(k_h, t) # Apply RoPE to key\n",
    "                score = sum(q_h[j] * k_h[j] for j in range(head_dim)) / head_dim**0.5\n",
    "                global_scores.append((t, score))\n",
    "            \n",
    "            # Select top-k global positions based on attention scores\n",
    "            global_scores.sort(key=lambda x: x[1].data, reverse=True)\n",
    "            top_k_positions = global_scores[:top_k_global] if len(global_scores) > 0 else []\n",
    "            \n",
    "            # Combine local and top-k global positions\n",
    "            selected_positions = local_scores + top_k_positions\n",
    "            selected_positions.sort(key=lambda x: x[0]) # sort by position for causality\n",
    "            \n",
    "            # Compute softmax over selected positions only (sparse attention)\n",
    "            attn_logits = [score for _, score in selected_positions]\n",
    "            attn_weights = softmax(attn_logits)\n",
    "            \n",
    "            # Weighted sum of values at selected positions\n",
    "            head_out = []\n",
    "            for j in range(head_dim):\n",
    "                weighted_sum = Value(0)\n",
    "                for idx, (t, _) in enumerate(selected_positions):\n",
    "                    v_h_j = values[li][t][hs+j]\n",
    "                    weighted_sum = weighted_sum + attn_weights[idx] * v_h_j\n",
    "                head_out.append(weighted_sum)\n",
    "            \n",
    "            x_attn.extend(head_out)\n",
    "        \n",
    "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "        \n",
    "        # 2) MLP block\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
    "        x = [xi.relu() for xi in x]\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "\n",
    "    logits = linear(x, state_dict['lm_head'])\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5d382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 / 1000 | loss 3.4876\n",
      "step    2 / 1000 | loss 3.4306\n",
      "step    3 / 1000 | loss 3.3536\n",
      "step    4 / 1000 | loss 3.3815\n",
      "step    5 / 1000 | loss 3.3145\n",
      "step    6 / 1000 | loss 3.3011\n",
      "step    7 / 1000 | loss 3.2203\n",
      "step    8 / 1000 | loss 3.2358\n",
      "step    9 / 1000 | loss 3.0648\n",
      "step   10 / 1000 | loss 3.3749\n",
      "step   11 / 1000 | loss 3.0410\n",
      "step   12 / 1000 | loss 3.0969\n",
      "step   13 / 1000 | loss 3.2723\n",
      "step   14 / 1000 | loss 3.0880\n",
      "step   15 / 1000 | loss 3.0985\n",
      "step   16 / 1000 | loss 2.7939\n",
      "step   17 / 1000 | loss 2.9402\n",
      "step   18 / 1000 | loss 3.2151\n",
      "step   19 / 1000 | loss 2.8140\n",
      "step   20 / 1000 | loss 2.9276\n",
      "step   21 / 1000 | loss 3.1479\n",
      "step   22 / 1000 | loss 3.0873\n",
      "step   23 / 1000 | loss 2.9876\n",
      "step   24 / 1000 | loss 2.3245\n",
      "step   25 / 1000 | loss 3.7202\n",
      "step   26 / 1000 | loss 2.5938\n",
      "step   27 / 1000 | loss 3.4591\n",
      "step   28 / 1000 | loss 2.8664\n",
      "step   29 / 1000 | loss 2.4308\n",
      "step   30 / 1000 | loss 2.6233\n",
      "step   31 / 1000 | loss 2.9689\n",
      "step   32 / 1000 | loss 2.9488\n",
      "step   33 / 1000 | loss 2.8225\n",
      "step   34 / 1000 | loss 2.4732\n",
      "step   35 / 1000 | loss 3.2896\n",
      "step   36 / 1000 | loss 3.1300\n",
      "step   37 / 1000 | loss 2.7537\n",
      "step   38 / 1000 | loss 2.6416\n",
      "step   39 / 1000 | loss 2.6766\n",
      "step   40 / 1000 | loss 2.8960\n",
      "step   41 / 1000 | loss 2.9429\n",
      "step   42 / 1000 | loss 2.7571\n",
      "step   43 / 1000 | loss 2.8704\n",
      "step   44 / 1000 | loss 2.6073\n",
      "step   45 / 1000 | loss 2.3869\n",
      "step   46 / 1000 | loss 2.7265\n",
      "step   47 / 1000 | loss 2.8568\n",
      "step   48 / 1000 | loss 3.2342\n",
      "step   49 / 1000 | loss 2.8113\n",
      "step   50 / 1000 | loss 2.3929\n",
      "step   51 / 1000 | loss 3.4220\n",
      "step   52 / 1000 | loss 2.6440\n",
      "step   53 / 1000 | loss 3.1691\n",
      "step   54 / 1000 | loss 2.4043\n",
      "step   55 / 1000 | loss 2.7832\n",
      "step   56 / 1000 | loss 2.3549\n",
      "step   57 / 1000 | loss 2.5070\n",
      "step   58 / 1000 | loss 2.1630\n",
      "step   59 / 1000 | loss 2.9183\n",
      "step   60 / 1000 | loss 2.7877\n",
      "step   61 / 1000 | loss 2.3156\n",
      "step   62 / 1000 | loss 2.3454\n",
      "step   63 / 1000 | loss 2.6649\n",
      "step   64 / 1000 | loss 2.7343\n",
      "step   65 / 1000 | loss 2.7421\n",
      "step   66 / 1000 | loss 2.9857\n",
      "step   67 / 1000 | loss 2.8387\n",
      "step   68 / 1000 | loss 2.1942\n",
      "step   69 / 1000 | loss 2.4552\n",
      "step   70 / 1000 | loss 3.0233\n",
      "step   71 / 1000 | loss 3.2935\n",
      "step   72 / 1000 | loss 2.5500\n",
      "step   73 / 1000 | loss 2.6886\n",
      "step   74 / 1000 | loss 2.6597\n",
      "step   75 / 1000 | loss 2.1483\n",
      "step   76 / 1000 | loss 2.9193\n",
      "step   77 / 1000 | loss 3.0572\n",
      "step   78 / 1000 | loss 3.3819\n",
      "step   79 / 1000 | loss 2.5223\n",
      "step   80 / 1000 | loss 2.5807\n",
      "step   81 / 1000 | loss 2.3558\n",
      "step   82 / 1000 | loss 3.3039\n",
      "step   83 / 1000 | loss 2.6169\n",
      "step   84 / 1000 | loss 2.5798\n",
      "step   85 / 1000 | loss 2.4381\n",
      "step   86 / 1000 | loss 2.4050\n",
      "step   87 / 1000 | loss 2.7851\n",
      "step   88 / 1000 | loss 3.0962\n",
      "step   89 / 1000 | loss 2.7144\n",
      "step   90 / 1000 | loss 2.5911\n",
      "step   91 / 1000 | loss 2.4303\n",
      "step   92 / 1000 | loss 3.1126\n",
      "step   93 / 1000 | loss 2.6968\n",
      "step   94 / 1000 | loss 2.7500\n",
      "step   95 / 1000 | loss 2.7754\n",
      "step   96 / 1000 | loss 2.5576\n",
      "step   97 / 1000 | loss 2.2471\n",
      "step   98 / 1000 | loss 3.1625\n",
      "step   99 / 1000 | loss 2.2852\n",
      "step  100 / 1000 | loss 3.3975 | attending to ~6.0/9 tokens (66.7% density)\n",
      "step  101 / 1000 | loss 2.5600\n",
      "step  102 / 1000 | loss 2.3021\n",
      "step  103 / 1000 | loss 2.7806\n",
      "step  104 / 1000 | loss 2.6721\n",
      "step  105 / 1000 | loss 2.6664\n",
      "step  106 / 1000 | loss 2.3471\n",
      "step  107 / 1000 | loss 3.0056\n",
      "step  108 / 1000 | loss 2.5406\n",
      "step  109 / 1000 | loss 2.1817\n",
      "step  110 / 1000 | loss 2.5955\n",
      "step  111 / 1000 | loss 2.5008\n",
      "step  112 / 1000 | loss 2.7126\n",
      "step  113 / 1000 | loss 3.1039\n",
      "step  114 / 1000 | loss 2.3527\n",
      "step  115 / 1000 | loss 2.2710\n",
      "step  116 / 1000 | loss 2.8099\n",
      "step  117 / 1000 | loss 2.7631\n",
      "step  118 / 1000 | loss 2.0543\n",
      "step  119 / 1000 | loss 2.5796\n",
      "step  120 / 1000 | loss 2.7722\n",
      "step  121 / 1000 | loss 2.7519\n",
      "step  122 / 1000 | loss 2.3068\n",
      "step  123 / 1000 | loss 2.9853\n",
      "step  124 / 1000 | loss 2.6330\n",
      "step  125 / 1000 | loss 3.3456\n",
      "step  126 / 1000 | loss 2.1862\n",
      "step  127 / 1000 | loss 2.7836\n",
      "step  128 / 1000 | loss 2.3291\n",
      "step  129 / 1000 | loss 2.3354\n",
      "step  130 / 1000 | loss 3.0960\n",
      "step  131 / 1000 | loss 3.0184\n",
      "step  132 / 1000 | loss 2.1583\n",
      "step  133 / 1000 | loss 3.1107\n",
      "step  134 / 1000 | loss 2.5649\n",
      "step  135 / 1000 | loss 2.5707\n",
      "step  136 / 1000 | loss 2.8874\n",
      "step  137 / 1000 | loss 2.1444\n",
      "step  138 / 1000 | loss 2.5455\n",
      "step  139 / 1000 | loss 3.0244\n",
      "step  140 / 1000 | loss 2.4038\n",
      "step  141 / 1000 | loss 3.5092\n",
      "step  142 / 1000 | loss 2.4880\n",
      "step  143 / 1000 | loss 3.2707\n",
      "step  144 / 1000 | loss 2.6133\n",
      "step  145 / 1000 | loss 2.7771\n",
      "step  146 / 1000 | loss 2.4543\n",
      "step  147 / 1000 | loss 2.1979\n",
      "step  148 / 1000 | loss 2.2079\n",
      "step  149 / 1000 | loss 2.4094\n",
      "step  150 / 1000 | loss 2.4540\n",
      "step  151 / 1000 | loss 3.2408\n",
      "step  152 / 1000 | loss 2.6220\n",
      "step  153 / 1000 | loss 2.8949\n",
      "step  154 / 1000 | loss 2.9899\n",
      "step  155 / 1000 | loss 2.6101\n",
      "step  156 / 1000 | loss 2.6461\n",
      "step  157 / 1000 | loss 3.1939\n",
      "step  158 / 1000 | loss 1.7721\n",
      "step  159 / 1000 | loss 3.3305\n",
      "step  160 / 1000 | loss 2.0704\n",
      "step  161 / 1000 | loss 2.1581\n",
      "step  162 / 1000 | loss 2.6730\n",
      "step  163 / 1000 | loss 2.2914\n",
      "step  164 / 1000 | loss 2.5536\n",
      "step  165 / 1000 | loss 2.2605\n",
      "step  166 / 1000 | loss 2.9182\n",
      "step  167 / 1000 | loss 1.9309\n",
      "step  168 / 1000 | loss 2.4163\n",
      "step  169 / 1000 | loss 3.4867\n",
      "step  170 / 1000 | loss 2.6536\n",
      "step  171 / 1000 | loss 2.9197\n",
      "step  172 / 1000 | loss 2.5778\n",
      "step  173 / 1000 | loss 2.2391\n",
      "step  174 / 1000 | loss 3.0335\n",
      "step  175 / 1000 | loss 2.9484\n",
      "step  176 / 1000 | loss 2.6357\n",
      "step  177 / 1000 | loss 2.9962\n",
      "step  178 / 1000 | loss 2.4809\n",
      "step  179 / 1000 | loss 2.1645\n",
      "step  180 / 1000 | loss 2.4029\n",
      "step  181 / 1000 | loss 2.4460\n",
      "step  182 / 1000 | loss 2.0165\n",
      "step  183 / 1000 | loss 1.8518\n",
      "step  184 / 1000 | loss 3.1401\n",
      "step  185 / 1000 | loss 2.3597\n",
      "step  186 / 1000 | loss 2.5800\n",
      "step  187 / 1000 | loss 2.3296\n",
      "step  188 / 1000 | loss 2.4554\n",
      "step  189 / 1000 | loss 2.0332\n",
      "step  190 / 1000 | loss 2.5314\n",
      "step  191 / 1000 | loss 1.9338\n",
      "step  192 / 1000 | loss 2.6820\n",
      "step  193 / 1000 | loss 2.0965\n",
      "step  194 / 1000 | loss 2.7553\n",
      "step  195 / 1000 | loss 2.7858\n",
      "step  196 / 1000 | loss 2.4571\n",
      "step  197 / 1000 | loss 2.1872\n",
      "step  198 / 1000 | loss 2.9009\n",
      "step  199 / 1000 | loss 2.4888\n",
      "step  200 / 1000 | loss 2.4346 | attending to ~6.0/11 tokens (54.5% density)\n",
      "step  201 / 1000 | loss 2.3263\n",
      "step  202 / 1000 | loss 2.5191\n",
      "step  203 / 1000 | loss 2.2375\n",
      "step  204 / 1000 | loss 1.8880\n",
      "step  205 / 1000 | loss 2.7086\n",
      "step  206 / 1000 | loss 3.0258\n",
      "step  207 / 1000 | loss 2.9936\n",
      "step  208 / 1000 | loss 2.1923\n",
      "step  209 / 1000 | loss 2.3452\n",
      "step  210 / 1000 | loss 2.8273\n",
      "step  211 / 1000 | loss 1.9455\n",
      "step  212 / 1000 | loss 2.4695\n",
      "step  213 / 1000 | loss 4.1020\n",
      "step  214 / 1000 | loss 2.1603\n",
      "step  215 / 1000 | loss 2.7056\n",
      "step  216 / 1000 | loss 2.4897\n",
      "step  217 / 1000 | loss 2.1635\n",
      "step  218 / 1000 | loss 2.6103\n",
      "step  219 / 1000 | loss 2.4856\n",
      "step  220 / 1000 | loss 2.4192\n",
      "step  221 / 1000 | loss 3.3122\n",
      "step  222 / 1000 | loss 2.3317\n",
      "step  223 / 1000 | loss 2.1177\n",
      "step  224 / 1000 | loss 2.5609\n",
      "step  225 / 1000 | loss 2.5444\n",
      "step  226 / 1000 | loss 2.1501\n",
      "step  227 / 1000 | loss 2.5041\n",
      "step  228 / 1000 | loss 2.4439\n",
      "step  229 / 1000 | loss 3.1328\n",
      "step  230 / 1000 | loss 1.8888\n",
      "step  231 / 1000 | loss 2.7107\n",
      "step  232 / 1000 | loss 2.2710\n",
      "step  233 / 1000 | loss 2.0415\n",
      "step  234 / 1000 | loss 2.7614\n",
      "step  235 / 1000 | loss 1.8630\n",
      "step  236 / 1000 | loss 2.8798\n",
      "step  237 / 1000 | loss 2.4856\n",
      "step  238 / 1000 | loss 2.7398\n",
      "step  239 / 1000 | loss 2.4470\n",
      "step  240 / 1000 | loss 3.4828\n",
      "step  241 / 1000 | loss 2.9901\n",
      "step  242 / 1000 | loss 2.4032\n",
      "step  243 / 1000 | loss 2.3331\n",
      "step  244 / 1000 | loss 2.6085\n",
      "step  245 / 1000 | loss 2.3117\n",
      "step  246 / 1000 | loss 2.8308\n",
      "step  247 / 1000 | loss 2.5338\n",
      "step  248 / 1000 | loss 2.8025\n",
      "step  249 / 1000 | loss 2.1915\n",
      "step  250 / 1000 | loss 2.4418\n",
      "step  251 / 1000 | loss 2.1839\n",
      "step  252 / 1000 | loss 2.6581\n",
      "step  253 / 1000 | loss 2.6238\n",
      "step  254 / 1000 | loss 1.9548\n",
      "step  255 / 1000 | loss 2.1976\n",
      "step  256 / 1000 | loss 3.0810\n",
      "step  257 / 1000 | loss 2.1972\n",
      "step  258 / 1000 | loss 1.8206\n",
      "step  259 / 1000 | loss 2.1941\n",
      "step  260 / 1000 | loss 1.9516\n",
      "step  261 / 1000 | loss 2.3488\n",
      "step  262 / 1000 | loss 2.1302\n",
      "step  263 / 1000 | loss 3.0396\n",
      "step  264 / 1000 | loss 3.1559\n",
      "step  265 / 1000 | loss 2.1265\n",
      "step  266 / 1000 | loss 2.8398\n",
      "step  267 / 1000 | loss 2.0587\n",
      "step  268 / 1000 | loss 2.4203\n",
      "step  269 / 1000 | loss 2.1886\n",
      "step  270 / 1000 | loss 2.7904\n",
      "step  271 / 1000 | loss 2.2588\n",
      "step  272 / 1000 | loss 1.8970\n",
      "step  273 / 1000 | loss 2.7560\n",
      "step  274 / 1000 | loss 3.0130\n",
      "step  275 / 1000 | loss 2.0946\n",
      "step  276 / 1000 | loss 2.5927\n",
      "step  277 / 1000 | loss 3.0697\n",
      "step  278 / 1000 | loss 2.5489\n",
      "step  279 / 1000 | loss 2.2247\n",
      "step  280 / 1000 | loss 1.9778\n",
      "step  281 / 1000 | loss 2.9656\n",
      "step  282 / 1000 | loss 2.0181\n",
      "step  283 / 1000 | loss 2.4932\n",
      "step  284 / 1000 | loss 2.7000\n",
      "step  285 / 1000 | loss 3.4646\n",
      "step  286 / 1000 | loss 1.9889\n",
      "step  287 / 1000 | loss 2.3601\n",
      "step  288 / 1000 | loss 2.6713\n",
      "step  289 / 1000 | loss 2.2938\n",
      "step  290 / 1000 | loss 2.1879\n",
      "step  291 / 1000 | loss 3.2241\n",
      "step  292 / 1000 | loss 2.4240\n",
      "step  293 / 1000 | loss 1.9727\n",
      "step  294 / 1000 | loss 2.5688\n",
      "step  295 / 1000 | loss 2.7615\n",
      "step  296 / 1000 | loss 1.9695\n",
      "step  297 / 1000 | loss 2.3993\n",
      "step  298 / 1000 | loss 2.6779\n",
      "step  299 / 1000 | loss 2.2524\n",
      "step  300 / 1000 | loss 2.3154 | attending to ~6.0/6 tokens (100.0% density)\n",
      "step  301 / 1000 | loss 2.7430\n",
      "step  302 / 1000 | loss 2.1793\n",
      "step  303 / 1000 | loss 2.8558\n",
      "step  304 / 1000 | loss 2.3086\n",
      "step  305 / 1000 | loss 2.9112\n",
      "step  306 / 1000 | loss 2.3479\n",
      "step  307 / 1000 | loss 2.0418\n",
      "step  308 / 1000 | loss 2.4094\n",
      "step  309 / 1000 | loss 2.1551\n",
      "step  310 / 1000 | loss 2.6006\n",
      "step  311 / 1000 | loss 3.1710\n",
      "step  312 / 1000 | loss 1.9047\n",
      "step  313 / 1000 | loss 1.9112\n",
      "step  314 / 1000 | loss 2.3948\n",
      "step  315 / 1000 | loss 2.5147\n",
      "step  316 / 1000 | loss 1.8885\n",
      "step  317 / 1000 | loss 2.0275\n",
      "step  318 / 1000 | loss 2.0206\n",
      "step  319 / 1000 | loss 1.9073\n",
      "step  320 / 1000 | loss 1.9798\n",
      "step  321 / 1000 | loss 2.2953\n",
      "step  322 / 1000 | loss 2.5366\n",
      "step  323 / 1000 | loss 3.0949\n",
      "step  324 / 1000 | loss 3.0575\n",
      "step  325 / 1000 | loss 2.4055\n",
      "step  326 / 1000 | loss 2.1383\n",
      "step  327 / 1000 | loss 2.3444\n",
      "step  328 / 1000 | loss 1.7425\n",
      "step  329 / 1000 | loss 2.2259\n",
      "step  330 / 1000 | loss 2.3691\n",
      "step  331 / 1000 | loss 2.3572\n",
      "step  332 / 1000 | loss 2.6948\n",
      "step  333 / 1000 | loss 1.8983\n",
      "step  334 / 1000 | loss 2.9546\n",
      "step  335 / 1000 | loss 3.5847\n",
      "step  336 / 1000 | loss 3.3549\n",
      "step  337 / 1000 | loss 2.7721\n",
      "step  338 / 1000 | loss 2.7772\n",
      "step  339 / 1000 | loss 2.3501\n",
      "step  340 / 1000 | loss 2.1792\n",
      "step  341 / 1000 | loss 3.1583\n",
      "step  342 / 1000 | loss 2.6716\n",
      "step  343 / 1000 | loss 1.8880\n",
      "step  344 / 1000 | loss 2.7836\n",
      "step  345 / 1000 | loss 2.2434\n",
      "step  346 / 1000 | loss 2.6103\n",
      "step  347 / 1000 | loss 2.7712\n",
      "step  348 / 1000 | loss 2.7600\n",
      "step  349 / 1000 | loss 2.7132\n",
      "step  350 / 1000 | loss 2.1480\n",
      "step  351 / 1000 | loss 2.9984\n",
      "step  352 / 1000 | loss 2.1547\n",
      "step  353 / 1000 | loss 2.5052\n",
      "step  354 / 1000 | loss 2.8046\n",
      "step  355 / 1000 | loss 2.8740\n",
      "step  356 / 1000 | loss 2.1942\n",
      "step  357 / 1000 | loss 3.1042\n",
      "step  358 / 1000 | loss 2.0690\n",
      "step  359 / 1000 | loss 2.1586\n",
      "step  360 / 1000 | loss 2.5876\n",
      "step  361 / 1000 | loss 2.5447\n",
      "step  362 / 1000 | loss 2.5830\n",
      "step  363 / 1000 | loss 1.9219\n",
      "step  364 / 1000 | loss 2.2509\n",
      "step  365 / 1000 | loss 2.5318\n",
      "step  366 / 1000 | loss 2.1741\n",
      "step  367 / 1000 | loss 3.1829\n",
      "step  368 / 1000 | loss 2.9499\n",
      "step  369 / 1000 | loss 2.7522\n",
      "step  370 / 1000 | loss 2.2282\n",
      "step  371 / 1000 | loss 2.8522\n",
      "step  372 / 1000 | loss 2.3407\n",
      "step  373 / 1000 | loss 2.6033\n",
      "step  374 / 1000 | loss 2.0276\n",
      "step  375 / 1000 | loss 2.0179\n",
      "step  376 / 1000 | loss 2.5768\n",
      "step  377 / 1000 | loss 2.3790\n",
      "step  378 / 1000 | loss 2.1313\n",
      "step  379 / 1000 | loss 2.4603\n",
      "step  380 / 1000 | loss 2.0709\n",
      "step  381 / 1000 | loss 2.2651\n",
      "step  382 / 1000 | loss 2.4385\n",
      "step  383 / 1000 | loss 2.2591\n",
      "step  384 / 1000 | loss 3.2411\n",
      "step  385 / 1000 | loss 2.8401\n",
      "step  386 / 1000 | loss 2.4777\n",
      "step  387 / 1000 | loss 3.8416\n",
      "step  388 / 1000 | loss 2.3004\n",
      "step  389 / 1000 | loss 2.4094\n",
      "step  390 / 1000 | loss 2.7112\n",
      "step  391 / 1000 | loss 2.4723\n",
      "step  392 / 1000 | loss 2.0950\n",
      "step  393 / 1000 | loss 2.3055\n",
      "step  394 / 1000 | loss 2.4364\n",
      "step  395 / 1000 | loss 2.5902\n",
      "step  396 / 1000 | loss 1.9869\n",
      "step  397 / 1000 | loss 2.3468\n",
      "step  398 / 1000 | loss 2.7457\n",
      "step  399 / 1000 | loss 2.7416\n",
      "step  400 / 1000 | loss 2.2316 | attending to ~6.0/7 tokens (85.7% density)\n",
      "step  401 / 1000 | loss 2.6873\n",
      "step  402 / 1000 | loss 2.6726\n",
      "step  403 / 1000 | loss 2.0177\n",
      "step  404 / 1000 | loss 2.1716\n",
      "step  405 / 1000 | loss 2.7364\n",
      "step  406 / 1000 | loss 2.4663\n",
      "step  407 / 1000 | loss 2.3598\n",
      "step  408 / 1000 | loss 2.6805\n",
      "step  409 / 1000 | loss 2.8717\n",
      "step  410 / 1000 | loss 3.0127\n",
      "step  411 / 1000 | loss 2.2621\n",
      "step  412 / 1000 | loss 2.0695\n",
      "step  413 / 1000 | loss 2.3522\n",
      "step  414 / 1000 | loss 2.9173\n",
      "step  415 / 1000 | loss 2.1078\n",
      "step  416 / 1000 | loss 2.3743\n",
      "step  417 / 1000 | loss 3.1578\n",
      "step  418 / 1000 | loss 1.9638\n",
      "step  419 / 1000 | loss 2.2901\n",
      "step  420 / 1000 | loss 2.7479\n",
      "step  421 / 1000 | loss 2.5070\n",
      "step  422 / 1000 | loss 2.5903\n",
      "step  423 / 1000 | loss 2.1644\n",
      "step  424 / 1000 | loss 2.6177\n",
      "step  425 / 1000 | loss 2.6452\n",
      "step  426 / 1000 | loss 2.7351\n",
      "step  427 / 1000 | loss 2.6698\n",
      "step  428 / 1000 | loss 2.4254\n",
      "step  429 / 1000 | loss 2.1353\n",
      "step  430 / 1000 | loss 2.3906\n",
      "step  431 / 1000 | loss 2.3856\n",
      "step  432 / 1000 | loss 1.9606\n",
      "step  433 / 1000 | loss 2.3336\n",
      "step  434 / 1000 | loss 2.3932\n",
      "step  435 / 1000 | loss 2.6000\n",
      "step  436 / 1000 | loss 3.3250\n",
      "step  437 / 1000 | loss 2.7383\n",
      "step  438 / 1000 | loss 1.9619\n",
      "step  439 / 1000 | loss 2.0264\n",
      "step  440 / 1000 | loss 2.3181\n",
      "step  441 / 1000 | loss 2.7398\n",
      "step  442 / 1000 | loss 2.7680\n",
      "step  443 / 1000 | loss 3.0147\n",
      "step  444 / 1000 | loss 2.5067\n",
      "step  445 / 1000 | loss 2.4649\n",
      "step  446 / 1000 | loss 2.1662\n",
      "step  447 / 1000 | loss 2.2900\n",
      "step  448 / 1000 | loss 3.2178\n",
      "step  449 / 1000 | loss 2.1098\n",
      "step  450 / 1000 | loss 3.0655\n",
      "step  451 / 1000 | loss 2.7307\n",
      "step  452 / 1000 | loss 2.7253\n",
      "step  453 / 1000 | loss 2.5785\n",
      "step  454 / 1000 | loss 1.9247\n",
      "step  455 / 1000 | loss 2.2426\n",
      "step  456 / 1000 | loss 3.0127\n",
      "step  457 / 1000 | loss 2.5512\n",
      "step  458 / 1000 | loss 2.8418\n",
      "step  459 / 1000 | loss 1.7362\n",
      "step  460 / 1000 | loss 2.5081\n",
      "step  461 / 1000 | loss 1.9017\n",
      "step  462 / 1000 | loss 2.7731\n",
      "step  463 / 1000 | loss 2.2452\n",
      "step  464 / 1000 | loss 2.3074\n",
      "step  465 / 1000 | loss 2.6122\n",
      "step  466 / 1000 | loss 2.1126\n",
      "step  467 / 1000 | loss 2.1890\n",
      "step  468 / 1000 | loss 2.1227\n",
      "step  469 / 1000 | loss 2.7678\n",
      "step  470 / 1000 | loss 3.9016\n",
      "step  471 / 1000 | loss 2.6179\n",
      "step  472 / 1000 | loss 2.7986\n",
      "step  473 / 1000 | loss 2.7233\n",
      "step  474 / 1000 | loss 2.2866\n",
      "step  475 / 1000 | loss 2.3910\n",
      "step  476 / 1000 | loss 2.6297\n",
      "step  477 / 1000 | loss 3.0216\n",
      "step  478 / 1000 | loss 2.3763\n",
      "step  479 / 1000 | loss 2.2827\n",
      "step  480 / 1000 | loss 2.0908\n",
      "step  481 / 1000 | loss 2.8142\n",
      "step  482 / 1000 | loss 2.5980\n",
      "step  483 / 1000 | loss 2.5659\n",
      "step  484 / 1000 | loss 3.1000\n",
      "step  485 / 1000 | loss 2.5775\n",
      "step  486 / 1000 | loss 2.2724\n",
      "step  487 / 1000 | loss 2.8381\n",
      "step  488 / 1000 | loss 2.7789\n",
      "step  489 / 1000 | loss 2.3995\n",
      "step  490 / 1000 | loss 2.2824\n",
      "step  491 / 1000 | loss 2.3994\n",
      "step  492 / 1000 | loss 2.5809\n",
      "step  493 / 1000 | loss 2.5487\n",
      "step  494 / 1000 | loss 2.2253\n",
      "step  495 / 1000 | loss 2.5800\n",
      "step  496 / 1000 | loss 2.5432\n",
      "step  497 / 1000 | loss 2.8077\n",
      "step  498 / 1000 | loss 2.5420\n",
      "step  499 / 1000 | loss 2.2787\n",
      "step  500 / 1000 | loss 2.1762 | attending to ~6.0/7 tokens (85.7% density)\n",
      "step  501 / 1000 | loss 2.3564\n",
      "step  502 / 1000 | loss 2.0770\n",
      "step  503 / 1000 | loss 2.6893\n",
      "step  504 / 1000 | loss 2.1864\n",
      "step  505 / 1000 | loss 1.8988\n",
      "step  506 / 1000 | loss 2.1964\n",
      "step  507 / 1000 | loss 2.5233\n",
      "step  508 / 1000 | loss 2.5148\n",
      "step  509 / 1000 | loss 2.9542\n",
      "step  510 / 1000 | loss 2.5641\n",
      "step  511 / 1000 | loss 2.3452\n",
      "step  512 / 1000 | loss 2.0923\n",
      "step  513 / 1000 | loss 2.5640\n",
      "step  514 / 1000 | loss 2.9700\n",
      "step  515 / 1000 | loss 2.7511\n",
      "step  516 / 1000 | loss 2.9176\n",
      "step  517 / 1000 | loss 2.1303\n",
      "step  518 / 1000 | loss 2.3117\n",
      "step  519 / 1000 | loss 2.0499\n",
      "step  520 / 1000 | loss 2.8368\n",
      "step  521 / 1000 | loss 1.6408\n",
      "step  522 / 1000 | loss 1.9348\n",
      "step  523 / 1000 | loss 2.0873\n",
      "step  524 / 1000 | loss 2.3537\n",
      "step  525 / 1000 | loss 2.2537\n",
      "step  526 / 1000 | loss 2.5063\n",
      "step  527 / 1000 | loss 1.9159\n",
      "step  528 / 1000 | loss 2.5450\n",
      "step  529 / 1000 | loss 2.3403\n",
      "step  530 / 1000 | loss 1.6100\n",
      "step  531 / 1000 | loss 2.6007\n",
      "step  532 / 1000 | loss 2.2054\n",
      "step  533 / 1000 | loss 3.5957\n",
      "step  534 / 1000 | loss 2.1592\n",
      "step  535 / 1000 | loss 2.8680\n",
      "step  536 / 1000 | loss 1.8284\n",
      "step  537 / 1000 | loss 1.6464\n",
      "step  538 / 1000 | loss 2.5479\n",
      "step  539 / 1000 | loss 1.8918\n",
      "step  540 / 1000 | loss 2.5116\n",
      "step  541 / 1000 | loss 2.7247\n",
      "step  542 / 1000 | loss 2.0588\n",
      "step  543 / 1000 | loss 3.3858\n",
      "step  544 / 1000 | loss 2.3603\n",
      "step  545 / 1000 | loss 3.0185\n",
      "step  546 / 1000 | loss 2.6126\n",
      "step  547 / 1000 | loss 2.5074\n",
      "step  548 / 1000 | loss 2.9602\n",
      "step  549 / 1000 | loss 1.9898\n",
      "step  550 / 1000 | loss 2.0101\n",
      "step  551 / 1000 | loss 2.8745\n",
      "step  552 / 1000 | loss 2.3798\n",
      "step  553 / 1000 | loss 2.5985\n",
      "step  554 / 1000 | loss 2.8515\n",
      "step  555 / 1000 | loss 3.0972\n",
      "step  556 / 1000 | loss 2.2936\n",
      "step  557 / 1000 | loss 2.4737\n",
      "step  558 / 1000 | loss 2.2297\n",
      "step  559 / 1000 | loss 2.7190\n",
      "step  560 / 1000 | loss 3.0548\n",
      "step  561 / 1000 | loss 2.0091\n",
      "step  562 / 1000 | loss 3.0115\n",
      "step  563 / 1000 | loss 2.2008\n",
      "step  564 / 1000 | loss 2.1692\n",
      "step  565 / 1000 | loss 2.8437\n",
      "step  566 / 1000 | loss 1.6921\n",
      "step  567 / 1000 | loss 2.3640\n",
      "step  568 / 1000 | loss 3.0247\n",
      "step  569 / 1000 | loss 2.4236\n",
      "step  570 / 1000 | loss 2.7405\n",
      "step  571 / 1000 | loss 1.9324\n",
      "step  572 / 1000 | loss 3.0832\n",
      "step  573 / 1000 | loss 3.1526\n",
      "step  574 / 1000 | loss 2.5432\n",
      "step  575 / 1000 | loss 2.3384\n",
      "step  576 / 1000 | loss 2.1042\n",
      "step  577 / 1000 | loss 2.0904\n",
      "step  578 / 1000 | loss 2.5847\n",
      "step  579 / 1000 | loss 2.0875\n",
      "step  580 / 1000 | loss 2.2982\n",
      "step  581 / 1000 | loss 2.3782\n",
      "step  582 / 1000 | loss 2.4306\n",
      "step  583 / 1000 | loss 2.6658\n",
      "step  584 / 1000 | loss 2.9416\n",
      "step  585 / 1000 | loss 2.2426\n",
      "step  586 / 1000 | loss 2.3506\n",
      "step  587 / 1000 | loss 2.2730\n",
      "step  588 / 1000 | loss 2.2324\n",
      "step  589 / 1000 | loss 1.8035\n",
      "step  590 / 1000 | loss 2.4811\n",
      "step  591 / 1000 | loss 2.4366\n",
      "step  592 / 1000 | loss 2.3202\n",
      "step  593 / 1000 | loss 2.5462\n",
      "step  594 / 1000 | loss 2.8468\n",
      "step  595 / 1000 | loss 2.7013\n",
      "step  596 / 1000 | loss 1.9963\n",
      "step  597 / 1000 | loss 2.5267\n",
      "step  598 / 1000 | loss 2.1910\n",
      "step  599 / 1000 | loss 2.3869\n",
      "step  600 / 1000 | loss 2.2987 | attending to ~6.0/8 tokens (75.0% density)\n",
      "step  601 / 1000 | loss 2.0674\n",
      "step  602 / 1000 | loss 2.5975\n",
      "step  603 / 1000 | loss 2.6592\n",
      "step  604 / 1000 | loss 2.0846\n",
      "step  605 / 1000 | loss 2.6595\n",
      "step  606 / 1000 | loss 1.8374\n",
      "step  607 / 1000 | loss 2.4728\n",
      "step  608 / 1000 | loss 2.3996\n",
      "step  609 / 1000 | loss 2.0835\n",
      "step  610 / 1000 | loss 2.4779\n",
      "step  611 / 1000 | loss 2.5627\n",
      "step  612 / 1000 | loss 2.1434\n",
      "step  613 / 1000 | loss 2.0531\n",
      "step  614 / 1000 | loss 2.2631\n",
      "step  615 / 1000 | loss 2.3984\n",
      "step  616 / 1000 | loss 2.8855\n",
      "step  617 / 1000 | loss 2.6349\n",
      "step  618 / 1000 | loss 2.2731\n",
      "step  619 / 1000 | loss 2.6014\n",
      "step  620 / 1000 | loss 2.8336\n",
      "step  621 / 1000 | loss 1.8527\n",
      "step  622 / 1000 | loss 2.2070\n",
      "step  623 / 1000 | loss 2.0502\n",
      "step  624 / 1000 | loss 2.4657\n",
      "step  625 / 1000 | loss 1.8052\n",
      "step  626 / 1000 | loss 2.3807\n",
      "step  627 / 1000 | loss 2.3090\n",
      "step  628 / 1000 | loss 2.1548\n",
      "step  629 / 1000 | loss 2.6390\n",
      "step  630 / 1000 | loss 2.6212\n",
      "step  631 / 1000 | loss 2.9877\n",
      "step  632 / 1000 | loss 1.8901\n",
      "step  633 / 1000 | loss 2.6675\n",
      "step  634 / 1000 | loss 2.3513\n",
      "step  635 / 1000 | loss 3.4959\n",
      "step  636 / 1000 | loss 2.9244\n",
      "step  637 / 1000 | loss 2.5123\n",
      "step  638 / 1000 | loss 2.3856\n",
      "step  639 / 1000 | loss 2.7218\n",
      "step  640 / 1000 | loss 2.0065\n",
      "step  641 / 1000 | loss 2.3592\n",
      "step  642 / 1000 | loss 1.9040\n",
      "step  643 / 1000 | loss 2.7050\n",
      "step  644 / 1000 | loss 2.3009\n",
      "step  645 / 1000 | loss 2.2521\n",
      "step  646 / 1000 | loss 2.0607\n",
      "step  647 / 1000 | loss 2.1240\n",
      "step  648 / 1000 | loss 2.9701\n",
      "step  649 / 1000 | loss 2.3052\n",
      "step  650 / 1000 | loss 2.4581\n",
      "step  651 / 1000 | loss 2.5525\n",
      "step  652 / 1000 | loss 2.5002\n",
      "step  653 / 1000 | loss 2.6753\n",
      "step  654 / 1000 | loss 1.9348\n",
      "step  655 / 1000 | loss 3.0839\n",
      "step  656 / 1000 | loss 2.3408\n",
      "step  657 / 1000 | loss 2.4225\n",
      "step  658 / 1000 | loss 2.9388\n",
      "step  659 / 1000 | loss 2.8144\n",
      "step  660 / 1000 | loss 2.4402\n",
      "step  661 / 1000 | loss 2.0040\n",
      "step  662 / 1000 | loss 2.8988\n",
      "step  663 / 1000 | loss 2.6806\n",
      "step  664 / 1000 | loss 2.6221\n",
      "step  665 / 1000 | loss 2.4355\n",
      "step  666 / 1000 | loss 2.2704\n",
      "step  667 / 1000 | loss 3.0149\n",
      "step  668 / 1000 | loss 2.7541\n",
      "step  669 / 1000 | loss 2.0867\n",
      "step  670 / 1000 | loss 2.2911\n",
      "step  671 / 1000 | loss 2.2015\n",
      "step  672 / 1000 | loss 1.8892\n",
      "step  673 / 1000 | loss 2.5578\n",
      "step  674 / 1000 | loss 2.7026\n",
      "step  675 / 1000 | loss 2.6285\n",
      "step  676 / 1000 | loss 2.0457\n",
      "step  677 / 1000 | loss 2.1149\n",
      "step  678 / 1000 | loss 2.2416\n",
      "step  679 / 1000 | loss 2.0678\n",
      "step  680 / 1000 | loss 2.2558\n",
      "step  681 / 1000 | loss 2.9681\n",
      "step  682 / 1000 | loss 2.0791\n",
      "step  683 / 1000 | loss 1.7889\n",
      "step  684 / 1000 | loss 2.2987\n",
      "step  685 / 1000 | loss 2.0970\n",
      "step  686 / 1000 | loss 2.3484\n",
      "step  687 / 1000 | loss 1.9873\n",
      "step  688 / 1000 | loss 2.0996\n",
      "step  689 / 1000 | loss 2.2303\n",
      "step  690 / 1000 | loss 2.2767\n",
      "step  691 / 1000 | loss 2.0705\n",
      "step  692 / 1000 | loss 1.8279\n",
      "step  693 / 1000 | loss 2.0695\n",
      "step  694 / 1000 | loss 3.1795\n",
      "step  695 / 1000 | loss 2.5329\n",
      "step  696 / 1000 | loss 2.3004\n",
      "step  697 / 1000 | loss 3.1110\n",
      "step  698 / 1000 | loss 2.4377\n",
      "step  699 / 1000 | loss 2.6878\n",
      "step  700 / 1000 | loss 2.4455 | attending to ~6.0/6 tokens (100.0% density)\n",
      "step  701 / 1000 | loss 2.1660\n",
      "step  702 / 1000 | loss 2.7256\n",
      "step  703 / 1000 | loss 2.4866\n",
      "step  704 / 1000 | loss 2.1633\n",
      "step  705 / 1000 | loss 2.2231\n",
      "step  706 / 1000 | loss 2.2328\n",
      "step  707 / 1000 | loss 3.0458\n",
      "step  708 / 1000 | loss 2.3575\n",
      "step  709 / 1000 | loss 1.9227\n",
      "step  710 / 1000 | loss 2.2741\n",
      "step  711 / 1000 | loss 2.8206\n",
      "step  712 / 1000 | loss 2.3524\n",
      "step  713 / 1000 | loss 2.3886\n",
      "step  714 / 1000 | loss 2.4854\n",
      "step  715 / 1000 | loss 1.9107\n",
      "step  716 / 1000 | loss 1.8823\n",
      "step  717 / 1000 | loss 2.2255\n",
      "step  718 / 1000 | loss 2.2206\n",
      "step  719 / 1000 | loss 2.8632\n",
      "step  720 / 1000 | loss 2.2120\n",
      "step  721 / 1000 | loss 1.9580\n",
      "step  722 / 1000 | loss 2.6916\n",
      "step  723 / 1000 | loss 1.8493\n",
      "step  724 / 1000 | loss 2.8958\n",
      "step  725 / 1000 | loss 2.8146\n",
      "step  726 / 1000 | loss 2.7895\n",
      "step  727 / 1000 | loss 1.8782\n",
      "step  728 / 1000 | loss 2.2356\n",
      "step  729 / 1000 | loss 2.2432\n",
      "step  730 / 1000 | loss 2.3632\n",
      "step  731 / 1000 | loss 2.5922\n",
      "step  732 / 1000 | loss 2.4158\n",
      "step  733 / 1000 | loss 2.5212\n",
      "step  734 / 1000 | loss 2.8121\n",
      "step  735 / 1000 | loss 3.1223\n",
      "step  736 / 1000 | loss 2.3220\n",
      "step  737 / 1000 | loss 3.0036\n",
      "step  738 / 1000 | loss 1.9080\n",
      "step  739 / 1000 | loss 2.3089\n",
      "step  740 / 1000 | loss 3.3714\n",
      "step  741 / 1000 | loss 2.6549\n",
      "step  742 / 1000 | loss 2.8744\n",
      "step  743 / 1000 | loss 2.4271\n",
      "step  744 / 1000 | loss 2.5203\n",
      "step  745 / 1000 | loss 2.2135\n",
      "step  746 / 1000 | loss 2.8834\n",
      "step  747 / 1000 | loss 2.5481\n",
      "step  748 / 1000 | loss 3.0180\n",
      "step  749 / 1000 | loss 2.6548\n",
      "step  750 / 1000 | loss 2.0565\n",
      "step  751 / 1000 | loss 2.2509\n",
      "step  752 / 1000 | loss 1.9302\n",
      "step  753 / 1000 | loss 2.8235\n",
      "step  754 / 1000 | loss 2.6010\n",
      "step  755 / 1000 | loss 2.9127\n",
      "step  756 / 1000 | loss 2.7250\n",
      "step  757 / 1000 | loss 2.5363\n",
      "step  758 / 1000 | loss 2.2476\n",
      "step  759 / 1000 | loss 2.3394\n",
      "step  760 / 1000 | loss 2.6622\n",
      "step  761 / 1000 | loss 2.3197\n",
      "step  762 / 1000 | loss 2.1478\n",
      "step  763 / 1000 | loss 2.0737\n",
      "step  764 / 1000 | loss 1.9928\n",
      "step  765 / 1000 | loss 2.0671\n",
      "step  766 / 1000 | loss 2.3835\n",
      "step  767 / 1000 | loss 2.6093\n",
      "step  768 / 1000 | loss 2.6941\n",
      "step  769 / 1000 | loss 1.9535\n",
      "step  770 / 1000 | loss 2.2146\n",
      "step  771 / 1000 | loss 2.6753\n",
      "step  772 / 1000 | loss 2.0229\n",
      "step  773 / 1000 | loss 1.8146\n",
      "step  774 / 1000 | loss 2.5818\n",
      "step  775 / 1000 | loss 2.7055\n",
      "step  776 / 1000 | loss 1.8246\n",
      "step  777 / 1000 | loss 2.9229\n",
      "step  778 / 1000 | loss 1.9388\n",
      "step  779 / 1000 | loss 2.8348\n",
      "step  780 / 1000 | loss 1.9228\n",
      "step  781 / 1000 | loss 1.8061\n",
      "step  782 / 1000 | loss 2.9244\n",
      "step  783 / 1000 | loss 1.7767\n",
      "step  784 / 1000 | loss 2.3758\n",
      "step  785 / 1000 | loss 2.2789\n",
      "step  786 / 1000 | loss 2.0535\n",
      "step  787 / 1000 | loss 2.5205\n",
      "step  788 / 1000 | loss 2.8878\n",
      "step  789 / 1000 | loss 1.8816\n",
      "step  790 / 1000 | loss 2.2169\n",
      "step  791 / 1000 | loss 2.3093\n",
      "step  792 / 1000 | loss 2.1412\n",
      "step  793 / 1000 | loss 2.1618\n",
      "step  794 / 1000 | loss 2.2042\n",
      "step  795 / 1000 | loss 1.8612\n",
      "step  796 / 1000 | loss 2.1934\n",
      "step  797 / 1000 | loss 2.1003\n",
      "step  798 / 1000 | loss 2.7142\n",
      "step  799 / 1000 | loss 1.9268\n",
      "step  800 / 1000 | loss 2.3378 | attending to ~6.0/7 tokens (85.7% density)\n",
      "step  801 / 1000 | loss 2.1084\n",
      "step  802 / 1000 | loss 2.1427\n",
      "step  803 / 1000 | loss 1.8998\n",
      "step  804 / 1000 | loss 2.0341\n",
      "step  805 / 1000 | loss 2.6305\n",
      "step  806 / 1000 | loss 2.1344\n",
      "step  807 / 1000 | loss 2.1382\n",
      "step  808 / 1000 | loss 2.0079\n",
      "step  809 / 1000 | loss 2.2823\n",
      "step  810 / 1000 | loss 2.0343\n",
      "step  811 / 1000 | loss 2.1625\n",
      "step  812 / 1000 | loss 2.3247\n",
      "step  813 / 1000 | loss 2.5757\n",
      "step  814 / 1000 | loss 2.2573\n",
      "step  815 / 1000 | loss 2.3889\n",
      "step  816 / 1000 | loss 2.6266\n",
      "step  817 / 1000 | loss 1.7573\n",
      "step  818 / 1000 | loss 2.4990\n",
      "step  819 / 1000 | loss 2.0872\n",
      "step  820 / 1000 | loss 2.4581\n",
      "step  821 / 1000 | loss 2.1979\n",
      "step  822 / 1000 | loss 2.5738\n",
      "step  823 / 1000 | loss 1.6726\n",
      "step  824 / 1000 | loss 2.2080\n",
      "step  825 / 1000 | loss 2.6825\n",
      "step  826 / 1000 | loss 2.5104\n",
      "step  827 / 1000 | loss 2.6164\n",
      "step  828 / 1000 | loss 2.8105\n",
      "step  829 / 1000 | loss 1.8251\n",
      "step  830 / 1000 | loss 2.2718\n",
      "step  831 / 1000 | loss 1.9981\n",
      "step  832 / 1000 | loss 2.1160\n",
      "step  833 / 1000 | loss 1.9431\n",
      "step  834 / 1000 | loss 2.1631\n",
      "step  835 / 1000 | loss 2.7453\n",
      "step  836 / 1000 | loss 2.4807\n",
      "step  837 / 1000 | loss 3.4001\n",
      "step  838 / 1000 | loss 2.4119\n",
      "step  839 / 1000 | loss 2.0668\n",
      "step  840 / 1000 | loss 2.4346\n",
      "step  841 / 1000 | loss 2.8480\n",
      "step  842 / 1000 | loss 2.5110\n",
      "step  843 / 1000 | loss 1.8971\n",
      "step  844 / 1000 | loss 2.7091\n",
      "step  845 / 1000 | loss 2.5567\n",
      "step  846 / 1000 | loss 1.9534\n",
      "step  847 / 1000 | loss 2.8766\n",
      "step  848 / 1000 | loss 2.0932\n",
      "step  849 / 1000 | loss 1.6497\n",
      "step  850 / 1000 | loss 2.4854\n",
      "step  851 / 1000 | loss 2.7818\n",
      "step  852 / 1000 | loss 2.1246\n",
      "step  853 / 1000 | loss 2.2153\n",
      "step  854 / 1000 | loss 2.4096\n",
      "step  855 / 1000 | loss 2.2949\n",
      "step  856 / 1000 | loss 1.9943\n",
      "step  857 / 1000 | loss 2.6115\n",
      "step  858 / 1000 | loss 2.5770\n",
      "step  859 / 1000 | loss 2.0534\n",
      "step  860 / 1000 | loss 2.4558\n",
      "step  861 / 1000 | loss 2.5317\n",
      "step  862 / 1000 | loss 2.6437\n",
      "step  863 / 1000 | loss 2.4473\n",
      "step  864 / 1000 | loss 3.2740\n",
      "step  865 / 1000 | loss 2.0934\n",
      "step  866 / 1000 | loss 1.8964\n",
      "step  867 / 1000 | loss 2.1057\n",
      "step  868 / 1000 | loss 1.9289\n",
      "step  869 / 1000 | loss 2.5634\n",
      "step  870 / 1000 | loss 2.6681\n",
      "step  871 / 1000 | loss 2.1430\n",
      "step  872 / 1000 | loss 2.2137\n",
      "step  873 / 1000 | loss 2.6296\n",
      "step  874 / 1000 | loss 2.3515\n",
      "step  875 / 1000 | loss 2.3922\n",
      "step  876 / 1000 | loss 2.5542\n",
      "step  877 / 1000 | loss 2.0412\n",
      "step  878 / 1000 | loss 2.5866\n",
      "step  879 / 1000 | loss 2.2847\n",
      "step  880 / 1000 | loss 2.9236\n",
      "step  881 / 1000 | loss 2.4232\n",
      "step  882 / 1000 | loss 1.9419\n",
      "step  883 / 1000 | loss 2.2021\n",
      "step  884 / 1000 | loss 2.2078\n",
      "step  885 / 1000 | loss 2.2102\n",
      "step  886 / 1000 | loss 2.0433\n",
      "step  887 / 1000 | loss 2.9617\n",
      "step  888 / 1000 | loss 2.3186\n",
      "step  889 / 1000 | loss 2.8787\n",
      "step  890 / 1000 | loss 2.0316\n",
      "step  891 / 1000 | loss 2.1033\n",
      "step  892 / 1000 | loss 3.5401\n",
      "step  893 / 1000 | loss 2.0716\n",
      "step  894 / 1000 | loss 1.7432\n",
      "step  895 / 1000 | loss 2.0273\n",
      "step  896 / 1000 | loss 2.3584\n",
      "step  897 / 1000 | loss 2.2465\n",
      "step  898 / 1000 | loss 1.9781\n",
      "step  899 / 1000 | loss 2.2677\n",
      "step  900 / 1000 | loss 2.9129 | attending to ~6.0/6 tokens (100.0% density)\n",
      "step  901 / 1000 | loss 2.0253\n",
      "step  902 / 1000 | loss 2.3621\n",
      "step  903 / 1000 | loss 1.7574\n",
      "step  904 / 1000 | loss 1.9612\n",
      "step  905 / 1000 | loss 2.3454\n",
      "step  906 / 1000 | loss 2.0613\n",
      "step  907 / 1000 | loss 2.4166\n",
      "step  908 / 1000 | loss 2.3430\n",
      "step  909 / 1000 | loss 2.3137\n",
      "step  910 / 1000 | loss 2.8392\n",
      "step  911 / 1000 | loss 2.4513\n",
      "step  912 / 1000 | loss 2.8502\n",
      "step  913 / 1000 | loss 2.5380\n",
      "step  914 / 1000 | loss 3.0150\n",
      "step  915 / 1000 | loss 2.2995\n",
      "step  916 / 1000 | loss 2.1535\n",
      "step  917 / 1000 | loss 2.2749\n",
      "step  918 / 1000 | loss 2.2745\n",
      "step  919 / 1000 | loss 2.3894\n",
      "step  920 / 1000 | loss 2.7659\n",
      "step  921 / 1000 | loss 2.1057\n",
      "step  922 / 1000 | loss 1.9388\n",
      "step  923 / 1000 | loss 2.0774\n",
      "step  924 / 1000 | loss 2.3498\n",
      "step  925 / 1000 | loss 1.9457\n",
      "step  926 / 1000 | loss 1.9759\n",
      "step  927 / 1000 | loss 2.5357\n",
      "step  928 / 1000 | loss 2.4854\n",
      "step  929 / 1000 | loss 1.9717\n",
      "step  930 / 1000 | loss 2.4900\n",
      "step  931 / 1000 | loss 2.5742\n",
      "step  932 / 1000 | loss 2.4711\n",
      "step  933 / 1000 | loss 2.4913\n",
      "step  934 / 1000 | loss 2.1129\n",
      "step  935 / 1000 | loss 1.9676\n",
      "step  936 / 1000 | loss 1.8016\n",
      "step  937 / 1000 | loss 1.9729\n",
      "step  938 / 1000 | loss 1.6678\n",
      "step  939 / 1000 | loss 2.0083\n",
      "step  940 / 1000 | loss 1.6923\n",
      "step  941 / 1000 | loss 1.9084\n",
      "step  942 / 1000 | loss 2.1081\n",
      "step  943 / 1000 | loss 1.8678\n",
      "step  944 / 1000 | loss 2.8469\n",
      "step  945 / 1000 | loss 1.8470\n",
      "step  946 / 1000 | loss 2.3922\n",
      "step  947 / 1000 | loss 1.8408\n",
      "step  948 / 1000 | loss 1.8551\n",
      "step  949 / 1000 | loss 1.9116\n",
      "step  950 / 1000 | loss 2.5973\n",
      "step  951 / 1000 | loss 2.8004\n",
      "step  952 / 1000 | loss 2.0801\n",
      "step  953 / 1000 | loss 2.2472\n",
      "step  954 / 1000 | loss 2.1375\n",
      "step  955 / 1000 | loss 2.6888\n",
      "step  956 / 1000 | loss 3.3271\n",
      "step  957 / 1000 | loss 2.0799\n",
      "step  958 / 1000 | loss 2.1462\n",
      "step  959 / 1000 | loss 2.1033\n",
      "step  960 / 1000 | loss 2.2208\n",
      "step  961 / 1000 | loss 2.1835\n",
      "step  962 / 1000 | loss 2.3742\n",
      "step  963 / 1000 | loss 2.3469\n",
      "step  964 / 1000 | loss 2.6030\n",
      "step  965 / 1000 | loss 3.0365\n",
      "step  966 / 1000 | loss 2.0030\n",
      "step  967 / 1000 | loss 2.9494\n",
      "step  968 / 1000 | loss 1.7253\n",
      "step  969 / 1000 | loss 2.2699\n",
      "step  970 / 1000 | loss 1.9080\n",
      "step  971 / 1000 | loss 2.1404\n",
      "step  972 / 1000 | loss 1.9998\n",
      "step  973 / 1000 | loss 2.5033\n",
      "step  974 / 1000 | loss 2.0734\n",
      "step  975 / 1000 | loss 2.0903\n",
      "step  976 / 1000 | loss 1.9597\n",
      "step  977 / 1000 | loss 2.4331\n",
      "step  978 / 1000 | loss 1.9328\n",
      "step  979 / 1000 | loss 2.0914\n",
      "step  980 / 1000 | loss 1.9798\n",
      "step  981 / 1000 | loss 2.5067\n",
      "step  982 / 1000 | loss 2.9782\n",
      "step  983 / 1000 | loss 2.5334\n",
      "step  984 / 1000 | loss 2.0473\n",
      "step  985 / 1000 | loss 2.7988\n",
      "step  986 / 1000 | loss 2.7223\n",
      "step  987 / 1000 | loss 1.6881\n",
      "step  988 / 1000 | loss 2.4801\n",
      "step  989 / 1000 | loss 2.5091\n",
      "step  990 / 1000 | loss 2.5835\n",
      "step  991 / 1000 | loss 2.1491\n",
      "step  992 / 1000 | loss 1.9536\n",
      "step  993 / 1000 | loss 2.5432\n",
      "step  994 / 1000 | loss 1.9276\n",
      "step  995 / 1000 | loss 2.5610\n",
      "step  996 / 1000 | loss 2.1785\n",
      "step  997 / 1000 | loss 1.8463\n",
      "step  998 / 1000 | loss 2.4267\n",
      "step  999 / 1000 | loss 2.5251\n",
      "step 1000 / 1000 | loss 2.5376 | attending to ~5.0/5 tokens (100.0% density)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let there be Adam, the blessed optimizer and its buffers\n",
    "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
    "m = [0.0] * len(params) # first moment buffer\n",
    "v = [0.0] * len(params) # second moment buffer\n",
    "\n",
    "# Repeat in sequence\n",
    "num_steps = 1000 # number of training steps\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
    "    doc = docs[step % len(docs)]\n",
    "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
    "    n = min(block_size, len(tokens) - 1)\n",
    "\n",
    "    # Forward the token sequence through the model, building up the computation graph all the way to the loss.\n",
    "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
    "    losses = []\n",
    "    for pos_id in range(n):\n",
    "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
    "        logits = gpt(token_id, pos_id, keys, values)\n",
    "        probs = softmax(logits)\n",
    "        loss_t = -probs[target_id].log()\n",
    "        losses.append(loss_t)\n",
    "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
    "\n",
    "    # Backward the loss, calculating the gradients with respect to all model parameters.\n",
    "    loss.backward()\n",
    "\n",
    "    # Adam optimizer update: update the model parameters based on the corresponding gradients.\n",
    "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
    "    for i, p in enumerate(params):\n",
    "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
    "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
    "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
    "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
    "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
    "        p.grad = 0\n",
    "\n",
    "    # Report sparsity statistics every 100 steps\n",
    "    if (step + 1) % 100 == 0:\n",
    "        avg_attend = min(local_window, n) + min(top_k_global, max(0, n - local_window))\n",
    "        sparsity_ratio = avg_attend / max(1, n) if n > 0 else 1.0\n",
    "        print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f} | attending to ~{avg_attend:.1f}/{n} tokens ({sparsity_ratio*100:.1f}% density)\")\n",
    "    else:\n",
    "        print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3968de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- inference (new, hallucinated names with DSA + RoPE) ---\n",
      "sample  1: non\n",
      "sample  2: alya\n",
      "sample  3: alilon\n",
      "sample  4: jara\n",
      "sample  5: ranna\n",
      "sample  6: maise\n",
      "sample  7: jajan\n",
      "sample  8: maya\n",
      "sample  9: varer\n",
      "sample 10: janaya\n",
      "sample 11: salan\n",
      "sample 12: ayna\n",
      "sample 13: sorili\n",
      "sample 14: jena\n",
      "sample 15: arinna\n",
      "sample 16: inan\n",
      "sample 17: arilelin\n",
      "sample 18: biabi\n",
      "sample 19: karel\n",
      "sample 20: alia\n"
     ]
    }
   ],
   "source": [
    "# Inference: may the model babble back to us\n",
    "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
    "print(\"\\n--- inference (new, hallucinated names with DSA + RoPE) ---\")\n",
    "for sample_idx in range(20):\n",
    "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
    "    token_id = BOS\n",
    "    sample = []\n",
    "    for pos_id in range(block_size):\n",
    "        logits = gpt(token_id, pos_id, keys, values)\n",
    "        probs = softmax([l / temperature for l in logits])\n",
    "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
    "        if token_id == BOS:\n",
    "            break\n",
    "        sample.append(uchars[token_id])\n",
    "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ec465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
