{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1300e513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num docs:32033\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "if not os.path.exists('input.txt'):\n",
    "    names_url='https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
    "    urllib.request.urlretrieve(names_url, \"input.txt\")\n",
    "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()]\n",
    "random.shuffle(docs)\n",
    "print(f\"num docs:{len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cee0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16f8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokeniser\n",
    "uchars=sorted(set(''.join(docs)))\n",
    "BOS= len(uchars)\n",
    "vocab_size= BOS + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669bff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    __slots__ = ('data','grad','_children','_local_grads')\n",
    "\n",
    "    def __init__(self, data,children=(),local_grads=()):\n",
    "        self.data=data\n",
    "        self.grad=0\n",
    "        self._children = children\n",
    "        self._local_grads= local_grads\n",
    "    \n",
    "    def __add__(self,other): #x+3 x.__add__(3), x=Value(4), self.__add__(other)\n",
    "        # print(\"calling,__add__\")\n",
    "        other=other if isinstance(other,Value) else Value(other) # 3becomes Value(3)\n",
    "        return Value(self.data + other.data, (self,other), (1,1))\n",
    "    \n",
    "    def __radd__(self,others): #swaps 3+x to x+3\n",
    "        return self+others\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        # print(\"calling __multiply__\")\n",
    "        other=other if isinstance(other,Value) else Value(other)\n",
    "        return Value(self.data * other.data, (self,other),(other.data,self.data)) #deravitive of x=y.data, y=x.data\n",
    "    \n",
    "    def __rmul__(self, other): return self * other\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        return Value(\n",
    "            self.data ** other,\n",
    "            (self,),\n",
    "            (other * (self.data ** (other - 1)),)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def log(self): return Value(math.log(self.data),(self,),(1/self.data,))\n",
    "\n",
    "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self *-1\n",
    "    \n",
    "    def __sub__(self,other): return self +(-other)\n",
    "\n",
    "    def __truediv__(self,other): return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self,other): return other * self**-1\n",
    "\n",
    "    def relu(self):\n",
    "        return Value(max(0,self.data),(self,),(float(self.data >0),))\n",
    "    \n",
    "    '''\n",
    "    x ----\\\n",
    "        + ---- a ----\\\n",
    "y ----/               *\n",
    "                       \\\n",
    "                        z\n",
    "w ---------------------/\n",
    "z.grad=1\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def backward(self):\n",
    "        topo=[]\n",
    "        visited=set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad=1\n",
    "        for v in reversed(topo):\n",
    "            for child, local_grad in zip(v._children, v._local_grads):\n",
    "                child.grad+=local_grad*v.grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89197af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b       # c = 6.0\n",
    "L = c + a       # L = 8.0\n",
    "L.backward()\n",
    "print(a.grad)   # 4.0 (dL/da = b + 1 = 3 + 1, via both paths). L=ab+a\n",
    "print(b.grad)   # 2.0 (dL/db = a = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303d9a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters\n",
    "n_embed=16\n",
    "n_head=4\n",
    "n_layer=1\n",
    "block_size=16\n",
    "head_dim=n_embed//n_head\n",
    "'''shape = (B, 16, 16)\n",
    "         ↑   ↑   ↑\n",
    "         |   |   |\n",
    "         |   |   embedding dimension\n",
    "         |   sequence length(block_size)\n",
    "         batch\n",
    "'''\n",
    "matrix=lambda nout,nin,std=0.08:[\n",
    "    [Value(random.gauss(0,std)) for _ in range(nin)] \n",
    "    for _ in range (nout)\n",
    "    ]\n",
    "state_dict ={'wte':matrix(vocab_size,n_embed), 'wpe': matrix(block_size,n_embed),'lm_head': matrix(vocab_size,n_embed)}\n",
    "#wte = Word Token Embedding matrix\n",
    "#wpe = Word Position Embedding matrix\n",
    "\n",
    "for i in range(n_layer):\n",
    "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embed,n_embed)\n",
    "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embed,n_embed)\n",
    "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embed,n_embed)\n",
    "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embed,n_embed)\n",
    "    '''Input embeddings → produce Q, K, V\n",
    "\n",
    "Compute attention per head\n",
    "\n",
    "Concatenate all head outputs\n",
    "\n",
    "Apply a final linear projection'''\n",
    "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4*n_embed,n_embed)\n",
    "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embed,4*n_embed)\n",
    "\n",
    "parms = [p for mat in state_dict.values() for row in mat for p in row]\n",
    "\n",
    "len(parms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fd70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x,w): #transformation\n",
    "    return [sum(wi * xi for wi,xi in zip(wo,x)) for wo in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37357ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    max_val= max(val.data for val in logits)\n",
    "    exp= [(val-max_val).exp() for val in logits]\n",
    "    total= sum(exp)\n",
    "    return [e/total for e in exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209c3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm(x):\n",
    "    ms=sum(xi*xi for xi in x)/len(x)\n",
    "    scale=(ms+1e-5)**-0.5\n",
    "    return [xi * scale for xi in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491649b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdfb05ab",
   "metadata": {},
   "source": [
    "\n",
    "This is because modern transformers use **Pre-Norm architecture**.\n",
    "\n",
    "In a transformer layer, the structure looks like this:\n",
    "\n",
    "So per layer:\n",
    "\n",
    "* x→ Norm → Attention → Add residual\n",
    "\n",
    "Then:\n",
    "* Norm → MLP → Add residual\n",
    "\n",
    "So each layer has two normalizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99a5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(token_id, pos_id, keys, values):\n",
    "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
    "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
    "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    for li in range(n_layer):\n",
    "        # 1) Multi-head attention block\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
    "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
    "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
    "        keys[li].append(k)\n",
    "        values[li].append(v)\n",
    "        x_attn = []\n",
    "        for h in range(n_head):\n",
    "            hs = h * head_dim\n",
    "            q_h = q[hs:hs+head_dim]\n",
    "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
    "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
    "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
    "            attn_weights = softmax(attn_logits)\n",
    "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
    "            x_attn.extend(head_out)\n",
    "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "        # 2) MLP block\n",
    "        x_residual = x\n",
    "        x = rmsnorm(x)\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
    "        x = [xi.relu() for xi in x]\n",
    "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
    "        x = [a + b for a, b in zip(x, x_residual)]\n",
    "\n",
    "    logits = linear(x, state_dict['lm_head'])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53831b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 / 1000 | loss 3.4826\n",
      "step    2 / 1000 | loss 3.4193\n",
      "step    3 / 1000 | loss 3.2604\n",
      "step    4 / 1000 | loss 2.9972\n",
      "step    5 / 1000 | loss 3.3197\n",
      "step    6 / 1000 | loss 3.3560\n",
      "step    7 / 1000 | loss 3.2189\n",
      "step    8 / 1000 | loss 3.1300\n",
      "step    9 / 1000 | loss 3.1470\n",
      "step   10 / 1000 | loss 3.0812\n",
      "step   11 / 1000 | loss 3.1643\n",
      "step   12 / 1000 | loss 3.0338\n",
      "step   13 / 1000 | loss 3.2901\n",
      "step   14 / 1000 | loss 3.2390\n",
      "step   15 / 1000 | loss 2.9296\n",
      "step   16 / 1000 | loss 3.3146\n",
      "step   17 / 1000 | loss 3.1667\n",
      "step   18 / 1000 | loss 2.2688\n",
      "step   19 / 1000 | loss 3.0348\n",
      "step   20 / 1000 | loss 2.8102\n",
      "step   21 / 1000 | loss 3.3720\n",
      "step   22 / 1000 | loss 2.4961\n",
      "step   23 / 1000 | loss 2.9241\n",
      "step   24 / 1000 | loss 2.5861\n",
      "step   25 / 1000 | loss 2.3197\n",
      "step   26 / 1000 | loss 2.8641\n",
      "step   27 / 1000 | loss 3.1677\n",
      "step   28 / 1000 | loss 1.9265\n",
      "step   29 / 1000 | loss 2.8918\n",
      "step   30 / 1000 | loss 3.0055\n",
      "step   31 / 1000 | loss 2.5147\n",
      "step   32 / 1000 | loss 2.8298\n",
      "step   33 / 1000 | loss 2.6310\n",
      "step   34 / 1000 | loss 2.4467\n",
      "step   35 / 1000 | loss 2.8284\n",
      "step   36 / 1000 | loss 2.6742\n",
      "step   37 / 1000 | loss 3.2555\n",
      "step   38 / 1000 | loss 2.5826\n",
      "step   39 / 1000 | loss 3.2228\n",
      "step   40 / 1000 | loss 2.6038\n",
      "step   41 / 1000 | loss 2.4402\n",
      "step   42 / 1000 | loss 2.8160\n",
      "step   43 / 1000 | loss 3.2764\n",
      "step   44 / 1000 | loss 2.4671\n",
      "step   45 / 1000 | loss 2.7611\n",
      "step   46 / 1000 | loss 2.7546\n",
      "step   47 / 1000 | loss 2.5761\n",
      "step   48 / 1000 | loss 2.6413\n",
      "step   49 / 1000 | loss 2.5926\n",
      "step   50 / 1000 | loss 1.9107\n",
      "step   51 / 1000 | loss 2.1379\n",
      "step   52 / 1000 | loss 2.0421\n",
      "step   53 / 1000 | loss 2.7202\n",
      "step   54 / 1000 | loss 2.1754\n",
      "step   55 / 1000 | loss 3.4376\n",
      "step   56 / 1000 | loss 2.4605\n",
      "step   57 / 1000 | loss 3.0265\n",
      "step   58 / 1000 | loss 1.7842\n",
      "step   59 / 1000 | loss 2.3343\n",
      "step   60 / 1000 | loss 2.3696\n",
      "step   61 / 1000 | loss 2.5792\n",
      "step   62 / 1000 | loss 1.7207\n",
      "step   63 / 1000 | loss 2.5848\n",
      "step   64 / 1000 | loss 2.2996\n",
      "step   65 / 1000 | loss 3.3499\n",
      "step   66 / 1000 | loss 2.5637\n",
      "step   67 / 1000 | loss 2.7384\n",
      "step   68 / 1000 | loss 2.4422\n",
      "step   69 / 1000 | loss 3.0812\n",
      "step   70 / 1000 | loss 2.4398\n",
      "step   71 / 1000 | loss 2.2158\n",
      "step   72 / 1000 | loss 2.3112\n",
      "step   73 / 1000 | loss 2.7303\n",
      "step   74 / 1000 | loss 3.6377\n",
      "step   75 / 1000 | loss 2.2997\n",
      "step   76 / 1000 | loss 3.1606\n",
      "step   77 / 1000 | loss 2.9448\n",
      "step   78 / 1000 | loss 3.0139\n",
      "step   79 / 1000 | loss 2.4634\n",
      "step   80 / 1000 | loss 2.0605\n",
      "step   81 / 1000 | loss 2.5983\n",
      "step   82 / 1000 | loss 2.7623\n",
      "step   83 / 1000 | loss 2.7246\n",
      "step   84 / 1000 | loss 2.7928\n",
      "step   85 / 1000 | loss 2.2938\n",
      "step   86 / 1000 | loss 2.6880\n",
      "step   87 / 1000 | loss 2.4577\n",
      "step   88 / 1000 | loss 2.4136\n",
      "step   89 / 1000 | loss 2.0967\n",
      "step   90 / 1000 | loss 2.6173\n",
      "step   91 / 1000 | loss 2.7363\n",
      "step   92 / 1000 | loss 2.1854\n",
      "step   93 / 1000 | loss 2.4128\n",
      "step   94 / 1000 | loss 2.2760\n",
      "step   95 / 1000 | loss 2.0548\n",
      "step   96 / 1000 | loss 2.4649\n",
      "step   97 / 1000 | loss 2.7397\n",
      "step   98 / 1000 | loss 2.3450\n",
      "step   99 / 1000 | loss 3.2105\n",
      "step  100 / 1000 | loss 2.9565\n",
      "step  101 / 1000 | loss 2.5459\n",
      "step  102 / 1000 | loss 2.3374\n",
      "step  103 / 1000 | loss 2.2508\n",
      "step  104 / 1000 | loss 3.5352\n",
      "step  105 / 1000 | loss 2.8875\n",
      "step  106 / 1000 | loss 2.0489\n",
      "step  107 / 1000 | loss 2.4862\n",
      "step  108 / 1000 | loss 2.8785\n",
      "step  109 / 1000 | loss 3.3954\n",
      "step  110 / 1000 | loss 2.4952\n",
      "step  111 / 1000 | loss 2.5835\n",
      "step  112 / 1000 | loss 2.4717\n",
      "step  113 / 1000 | loss 1.8126\n",
      "step  114 / 1000 | loss 2.5353\n",
      "step  115 / 1000 | loss 2.3455\n",
      "step  116 / 1000 | loss 2.1659\n",
      "step  117 / 1000 | loss 2.5537\n",
      "step  118 / 1000 | loss 2.2163\n",
      "step  119 / 1000 | loss 2.3499\n",
      "step  120 / 1000 | loss 2.5495\n",
      "step  121 / 1000 | loss 2.8346\n",
      "step  122 / 1000 | loss 2.1020\n",
      "step  123 / 1000 | loss 2.2606\n",
      "step  124 / 1000 | loss 3.1635\n",
      "step  125 / 1000 | loss 2.1266\n",
      "step  126 / 1000 | loss 2.1555\n",
      "step  127 / 1000 | loss 2.1103\n",
      "step  128 / 1000 | loss 2.6159\n",
      "step  129 / 1000 | loss 2.5843\n",
      "step  130 / 1000 | loss 2.7521\n",
      "step  131 / 1000 | loss 3.0827\n",
      "step  132 / 1000 | loss 2.0839\n",
      "step  133 / 1000 | loss 2.7678\n",
      "step  134 / 1000 | loss 2.6256\n",
      "step  135 / 1000 | loss 2.2052\n",
      "step  136 / 1000 | loss 2.3726\n",
      "step  137 / 1000 | loss 1.9597\n",
      "step  138 / 1000 | loss 2.2775\n",
      "step  139 / 1000 | loss 2.9528\n",
      "step  140 / 1000 | loss 2.3412\n",
      "step  141 / 1000 | loss 2.0966\n",
      "step  142 / 1000 | loss 3.3233\n",
      "step  143 / 1000 | loss 2.9918\n",
      "step  144 / 1000 | loss 1.9664\n",
      "step  145 / 1000 | loss 2.1857\n",
      "step  146 / 1000 | loss 2.9498\n",
      "step  147 / 1000 | loss 2.0420\n",
      "step  148 / 1000 | loss 2.5096\n",
      "step  149 / 1000 | loss 2.5335\n",
      "step  150 / 1000 | loss 2.3994\n",
      "step  151 / 1000 | loss 2.0947\n",
      "step  152 / 1000 | loss 2.5407\n",
      "step  153 / 1000 | loss 1.7826\n",
      "step  154 / 1000 | loss 3.4806\n",
      "step  155 / 1000 | loss 3.2851\n",
      "step  156 / 1000 | loss 1.9166\n",
      "step  157 / 1000 | loss 2.0057\n",
      "step  158 / 1000 | loss 2.2873\n",
      "step  159 / 1000 | loss 2.9019\n",
      "step  160 / 1000 | loss 2.9012\n",
      "step  161 / 1000 | loss 2.0101\n",
      "step  162 / 1000 | loss 2.6221\n",
      "step  163 / 1000 | loss 2.1656\n",
      "step  164 / 1000 | loss 2.4843\n",
      "step  165 / 1000 | loss 1.8855\n",
      "step  166 / 1000 | loss 2.8991\n",
      "step  167 / 1000 | loss 2.2729\n",
      "step  168 / 1000 | loss 2.4037\n",
      "step  169 / 1000 | loss 2.6510\n",
      "step  170 / 1000 | loss 2.8392\n",
      "step  171 / 1000 | loss 2.5892\n",
      "step  172 / 1000 | loss 2.3505\n",
      "step  173 / 1000 | loss 2.1824\n",
      "step  174 / 1000 | loss 2.6317\n",
      "step  175 / 1000 | loss 2.8675\n",
      "step  176 / 1000 | loss 2.4804\n",
      "step  177 / 1000 | loss 2.3871\n",
      "step  178 / 1000 | loss 2.7556\n",
      "step  179 / 1000 | loss 2.1379\n",
      "step  180 / 1000 | loss 2.7130\n",
      "step  181 / 1000 | loss 2.6701\n",
      "step  182 / 1000 | loss 1.8487\n",
      "step  183 / 1000 | loss 2.2823\n",
      "step  184 / 1000 | loss 2.1472\n",
      "step  185 / 1000 | loss 2.1366\n",
      "step  186 / 1000 | loss 2.6549\n",
      "step  187 / 1000 | loss 2.1606\n",
      "step  188 / 1000 | loss 2.8412\n",
      "step  189 / 1000 | loss 2.6212\n",
      "step  190 / 1000 | loss 2.4495\n",
      "step  191 / 1000 | loss 2.0457\n",
      "step  192 / 1000 | loss 2.4988\n",
      "step  193 / 1000 | loss 2.3180\n",
      "step  194 / 1000 | loss 2.5850\n",
      "step  195 / 1000 | loss 3.1896\n",
      "step  196 / 1000 | loss 2.3880\n",
      "step  197 / 1000 | loss 3.2492\n",
      "step  198 / 1000 | loss 2.6359\n",
      "step  199 / 1000 | loss 3.1903\n",
      "step  200 / 1000 | loss 2.0987\n",
      "step  201 / 1000 | loss 3.1214\n",
      "step  202 / 1000 | loss 1.8940\n",
      "step  203 / 1000 | loss 2.1193\n",
      "step  204 / 1000 | loss 2.8305\n",
      "step  205 / 1000 | loss 2.3846\n",
      "step  206 / 1000 | loss 2.3677\n",
      "step  207 / 1000 | loss 2.6096\n",
      "step  208 / 1000 | loss 2.8136\n",
      "step  209 / 1000 | loss 2.7041\n",
      "step  210 / 1000 | loss 2.2277\n",
      "step  211 / 1000 | loss 2.8258\n",
      "step  212 / 1000 | loss 2.2752\n",
      "step  213 / 1000 | loss 2.0756\n",
      "step  214 / 1000 | loss 1.7560\n",
      "step  215 / 1000 | loss 1.8422\n",
      "step  216 / 1000 | loss 2.1849\n",
      "step  217 / 1000 | loss 2.8618\n",
      "step  218 / 1000 | loss 2.2747\n",
      "step  219 / 1000 | loss 2.9329\n",
      "step  220 / 1000 | loss 3.1385\n",
      "step  221 / 1000 | loss 2.4575\n",
      "step  222 / 1000 | loss 2.2774\n",
      "step  223 / 1000 | loss 2.3836\n",
      "step  224 / 1000 | loss 2.9318\n",
      "step  225 / 1000 | loss 3.1565\n",
      "step  226 / 1000 | loss 2.5124\n",
      "step  227 / 1000 | loss 2.3808\n",
      "step  228 / 1000 | loss 2.2886\n",
      "step  229 / 1000 | loss 3.6477\n",
      "step  230 / 1000 | loss 2.8042\n",
      "step  231 / 1000 | loss 1.9696\n",
      "step  232 / 1000 | loss 2.7793\n",
      "step  233 / 1000 | loss 2.4404\n",
      "step  234 / 1000 | loss 2.2223\n",
      "step  235 / 1000 | loss 2.2084\n",
      "step  236 / 1000 | loss 1.9817\n",
      "step  237 / 1000 | loss 2.7320\n",
      "step  238 / 1000 | loss 2.2020\n",
      "step  239 / 1000 | loss 2.5829\n",
      "step  240 / 1000 | loss 2.3354\n",
      "step  241 / 1000 | loss 2.5459\n",
      "step  242 / 1000 | loss 2.1363\n",
      "step  243 / 1000 | loss 2.8286\n",
      "step  244 / 1000 | loss 2.1572\n",
      "step  245 / 1000 | loss 2.4301\n",
      "step  246 / 1000 | loss 2.0658\n",
      "step  247 / 1000 | loss 2.3750\n",
      "step  248 / 1000 | loss 2.0859\n",
      "step  249 / 1000 | loss 2.7428\n",
      "step  250 / 1000 | loss 1.9509\n",
      "step  251 / 1000 | loss 3.2371\n",
      "step  252 / 1000 | loss 2.1456\n",
      "step  253 / 1000 | loss 2.7370\n",
      "step  254 / 1000 | loss 2.5626\n",
      "step  255 / 1000 | loss 2.1479\n",
      "step  256 / 1000 | loss 2.1027\n",
      "step  257 / 1000 | loss 2.5949\n",
      "step  258 / 1000 | loss 2.4441\n",
      "step  259 / 1000 | loss 2.3842\n",
      "step  260 / 1000 | loss 2.2012\n",
      "step  261 / 1000 | loss 2.3133\n",
      "step  262 / 1000 | loss 1.9798\n",
      "step  263 / 1000 | loss 3.4325\n",
      "step  264 / 1000 | loss 2.1619\n",
      "step  265 / 1000 | loss 2.6211\n",
      "step  266 / 1000 | loss 2.0389\n",
      "step  267 / 1000 | loss 3.0418\n",
      "step  268 / 1000 | loss 2.0249\n",
      "step  269 / 1000 | loss 2.8359\n",
      "step  270 / 1000 | loss 2.4508\n",
      "step  271 / 1000 | loss 2.3388\n",
      "step  272 / 1000 | loss 4.1042\n",
      "step  273 / 1000 | loss 2.6073\n",
      "step  274 / 1000 | loss 2.1333\n",
      "step  275 / 1000 | loss 2.3700\n",
      "step  276 / 1000 | loss 3.2702\n",
      "step  277 / 1000 | loss 2.3084\n",
      "step  278 / 1000 | loss 1.8368\n",
      "step  279 / 1000 | loss 2.6491\n",
      "step  280 / 1000 | loss 2.4900\n",
      "step  281 / 1000 | loss 2.6043\n",
      "step  282 / 1000 | loss 2.5628\n",
      "step  283 / 1000 | loss 2.5648\n",
      "step  284 / 1000 | loss 2.2334\n",
      "step  285 / 1000 | loss 1.9136\n",
      "step  286 / 1000 | loss 2.1181\n",
      "step  287 / 1000 | loss 2.9361\n",
      "step  288 / 1000 | loss 2.5299\n",
      "step  289 / 1000 | loss 2.2349\n",
      "step  290 / 1000 | loss 3.0094\n",
      "step  291 / 1000 | loss 3.2072\n",
      "step  292 / 1000 | loss 2.9131\n",
      "step  293 / 1000 | loss 2.9647\n",
      "step  294 / 1000 | loss 2.7839\n",
      "step  295 / 1000 | loss 2.7059\n",
      "step  296 / 1000 | loss 2.2212\n",
      "step  297 / 1000 | loss 2.4422\n",
      "step  298 / 1000 | loss 1.9892\n",
      "step  299 / 1000 | loss 2.0037\n",
      "step  300 / 1000 | loss 3.1351\n",
      "step  301 / 1000 | loss 1.9970\n",
      "step  302 / 1000 | loss 2.6088\n",
      "step  303 / 1000 | loss 2.5192\n",
      "step  304 / 1000 | loss 2.0764\n",
      "step  305 / 1000 | loss 2.0419\n",
      "step  306 / 1000 | loss 2.9513\n",
      "step  307 / 1000 | loss 3.1020\n",
      "step  308 / 1000 | loss 2.6116\n",
      "step  309 / 1000 | loss 3.0927\n",
      "step  310 / 1000 | loss 3.6009\n",
      "step  311 / 1000 | loss 2.9078\n",
      "step  312 / 1000 | loss 2.5034\n",
      "step  313 / 1000 | loss 2.3595\n",
      "step  314 / 1000 | loss 2.7905\n",
      "step  315 / 1000 | loss 2.4750\n",
      "step  316 / 1000 | loss 2.3590\n",
      "step  317 / 1000 | loss 2.2670\n",
      "step  318 / 1000 | loss 2.7955\n",
      "step  319 / 1000 | loss 2.2528\n",
      "step  320 / 1000 | loss 2.5777\n",
      "step  321 / 1000 | loss 2.2412\n",
      "step  322 / 1000 | loss 2.5148\n",
      "step  323 / 1000 | loss 2.5394\n",
      "step  324 / 1000 | loss 2.5830\n",
      "step  325 / 1000 | loss 2.1111\n",
      "step  326 / 1000 | loss 2.3099\n",
      "step  327 / 1000 | loss 2.2811\n",
      "step  328 / 1000 | loss 3.0453\n",
      "step  329 / 1000 | loss 2.9318\n",
      "step  330 / 1000 | loss 2.9728\n",
      "step  331 / 1000 | loss 2.5463\n",
      "step  332 / 1000 | loss 2.5296\n",
      "step  333 / 1000 | loss 2.9721\n",
      "step  334 / 1000 | loss 2.2639\n",
      "step  335 / 1000 | loss 3.5578\n",
      "step  336 / 1000 | loss 2.8464\n",
      "step  337 / 1000 | loss 2.5383\n",
      "step  338 / 1000 | loss 2.5212\n",
      "step  339 / 1000 | loss 2.6019\n",
      "step  340 / 1000 | loss 2.3765\n",
      "step  341 / 1000 | loss 2.1049\n",
      "step  342 / 1000 | loss 2.3974\n",
      "step  343 / 1000 | loss 1.9796\n",
      "step  344 / 1000 | loss 2.2601\n",
      "step  345 / 1000 | loss 2.2639\n",
      "step  346 / 1000 | loss 2.5776\n",
      "step  347 / 1000 | loss 2.5961\n",
      "step  348 / 1000 | loss 2.5583\n",
      "step  349 / 1000 | loss 2.2700\n",
      "step  350 / 1000 | loss 2.4074\n",
      "step  351 / 1000 | loss 2.4075\n",
      "step  352 / 1000 | loss 2.6845\n",
      "step  353 / 1000 | loss 2.6660\n",
      "step  354 / 1000 | loss 2.1129\n",
      "step  355 / 1000 | loss 1.8978\n",
      "step  356 / 1000 | loss 2.0756\n",
      "step  357 / 1000 | loss 3.0387\n",
      "step  358 / 1000 | loss 2.1468\n",
      "step  359 / 1000 | loss 2.0627\n",
      "step  360 / 1000 | loss 2.2012\n",
      "step  361 / 1000 | loss 2.3377\n",
      "step  362 / 1000 | loss 2.0763\n",
      "step  363 / 1000 | loss 3.1585\n",
      "step  364 / 1000 | loss 2.5583\n",
      "step  365 / 1000 | loss 2.6043\n",
      "step  366 / 1000 | loss 2.6602\n",
      "step  367 / 1000 | loss 1.7542\n",
      "step  368 / 1000 | loss 2.4860\n",
      "step  369 / 1000 | loss 2.1962\n",
      "step  370 / 1000 | loss 2.7886\n",
      "step  371 / 1000 | loss 2.3582\n",
      "step  372 / 1000 | loss 2.4081\n",
      "step  373 / 1000 | loss 2.0783\n",
      "step  374 / 1000 | loss 1.7101\n",
      "step  375 / 1000 | loss 2.1023\n",
      "step  376 / 1000 | loss 2.1564\n",
      "step  377 / 1000 | loss 2.7037\n",
      "step  378 / 1000 | loss 1.8821\n",
      "step  379 / 1000 | loss 2.4077\n",
      "step  380 / 1000 | loss 1.9188\n",
      "step  381 / 1000 | loss 2.1880\n",
      "step  382 / 1000 | loss 3.0228\n",
      "step  383 / 1000 | loss 3.1550\n",
      "step  384 / 1000 | loss 2.6748\n",
      "step  385 / 1000 | loss 2.6672\n",
      "step  386 / 1000 | loss 2.7519\n",
      "step  387 / 1000 | loss 2.2798\n",
      "step  388 / 1000 | loss 2.3904\n",
      "step  389 / 1000 | loss 2.8636\n",
      "step  390 / 1000 | loss 2.2552\n",
      "step  391 / 1000 | loss 2.6810\n",
      "step  392 / 1000 | loss 3.0197\n",
      "step  393 / 1000 | loss 2.5910\n",
      "step  394 / 1000 | loss 3.0064\n",
      "step  395 / 1000 | loss 2.8539\n",
      "step  396 / 1000 | loss 2.1949\n",
      "step  397 / 1000 | loss 2.1571\n",
      "step  398 / 1000 | loss 2.4853\n",
      "step  399 / 1000 | loss 2.2791\n",
      "step  400 / 1000 | loss 2.1058\n",
      "step  401 / 1000 | loss 2.3264\n",
      "step  402 / 1000 | loss 1.9800\n",
      "step  403 / 1000 | loss 1.9916\n",
      "step  404 / 1000 | loss 2.1164\n",
      "step  405 / 1000 | loss 2.3892\n",
      "step  406 / 1000 | loss 2.8381\n",
      "step  407 / 1000 | loss 2.0921\n",
      "step  408 / 1000 | loss 3.1883\n",
      "step  409 / 1000 | loss 2.7163\n",
      "step  410 / 1000 | loss 2.9518\n",
      "step  411 / 1000 | loss 2.6837\n",
      "step  412 / 1000 | loss 2.8467\n",
      "step  413 / 1000 | loss 2.7256\n",
      "step  414 / 1000 | loss 2.9864\n",
      "step  415 / 1000 | loss 2.4042\n",
      "step  416 / 1000 | loss 2.2968\n",
      "step  417 / 1000 | loss 2.4977\n",
      "step  418 / 1000 | loss 2.7176\n",
      "step  419 / 1000 | loss 2.2848\n",
      "step  420 / 1000 | loss 2.4573\n",
      "step  421 / 1000 | loss 1.7752\n",
      "step  422 / 1000 | loss 2.5170\n",
      "step  423 / 1000 | loss 2.2404\n",
      "step  424 / 1000 | loss 1.9848\n",
      "step  425 / 1000 | loss 2.4107\n",
      "step  426 / 1000 | loss 2.4230\n",
      "step  427 / 1000 | loss 2.5889\n",
      "step  428 / 1000 | loss 2.1574\n",
      "step  429 / 1000 | loss 2.1998\n",
      "step  430 / 1000 | loss 2.4980\n",
      "step  431 / 1000 | loss 4.1470\n",
      "step  432 / 1000 | loss 1.7806\n",
      "step  433 / 1000 | loss 2.4811\n",
      "step  434 / 1000 | loss 2.2977\n",
      "step  435 / 1000 | loss 2.3776\n",
      "step  436 / 1000 | loss 2.1345\n",
      "step  437 / 1000 | loss 1.9869\n",
      "step  438 / 1000 | loss 2.2682\n",
      "step  439 / 1000 | loss 2.2717\n",
      "step  440 / 1000 | loss 2.3557\n",
      "step  441 / 1000 | loss 2.2443\n",
      "step  442 / 1000 | loss 2.3271\n",
      "step  443 / 1000 | loss 2.2922\n",
      "step  444 / 1000 | loss 1.9968\n",
      "step  445 / 1000 | loss 2.4688\n",
      "step  446 / 1000 | loss 2.3872\n",
      "step  447 / 1000 | loss 2.6776\n",
      "step  448 / 1000 | loss 2.1916\n",
      "step  449 / 1000 | loss 2.4029\n",
      "step  450 / 1000 | loss 2.7879\n",
      "step  451 / 1000 | loss 2.4366\n",
      "step  452 / 1000 | loss 2.0003\n",
      "step  453 / 1000 | loss 2.0861\n",
      "step  454 / 1000 | loss 1.8753\n",
      "step  455 / 1000 | loss 2.1472\n",
      "step  456 / 1000 | loss 2.2088\n",
      "step  457 / 1000 | loss 2.0478\n",
      "step  458 / 1000 | loss 1.9998\n",
      "step  459 / 1000 | loss 2.1150\n",
      "step  460 / 1000 | loss 2.5585\n",
      "step  461 / 1000 | loss 2.3743\n",
      "step  462 / 1000 | loss 2.4649\n",
      "step  463 / 1000 | loss 2.0060\n",
      "step  464 / 1000 | loss 2.0567\n",
      "step  465 / 1000 | loss 2.1063\n",
      "step  466 / 1000 | loss 3.7898\n",
      "step  467 / 1000 | loss 1.9877\n",
      "step  468 / 1000 | loss 2.6858\n",
      "step  469 / 1000 | loss 1.7272\n",
      "step  470 / 1000 | loss 2.2593\n",
      "step  471 / 1000 | loss 2.7484\n",
      "step  472 / 1000 | loss 2.0761\n",
      "step  473 / 1000 | loss 3.5132\n",
      "step  474 / 1000 | loss 2.5524\n",
      "step  475 / 1000 | loss 2.3926\n",
      "step  476 / 1000 | loss 2.0916\n",
      "step  477 / 1000 | loss 2.5185\n",
      "step  478 / 1000 | loss 3.0122\n",
      "step  479 / 1000 | loss 2.0990\n",
      "step  480 / 1000 | loss 2.3078\n",
      "step  481 / 1000 | loss 2.4816\n",
      "step  482 / 1000 | loss 1.9371\n",
      "step  483 / 1000 | loss 1.6711\n",
      "step  484 / 1000 | loss 2.6074\n",
      "step  485 / 1000 | loss 1.7301\n",
      "step  486 / 1000 | loss 3.3340\n",
      "step  487 / 1000 | loss 1.9795\n",
      "step  488 / 1000 | loss 2.7114\n",
      "step  489 / 1000 | loss 1.8443\n",
      "step  490 / 1000 | loss 2.6009\n",
      "step  491 / 1000 | loss 2.5990\n",
      "step  492 / 1000 | loss 2.5454\n",
      "step  493 / 1000 | loss 2.1793\n",
      "step  494 / 1000 | loss 2.6232\n",
      "step  495 / 1000 | loss 1.9987\n",
      "step  496 / 1000 | loss 3.0094\n",
      "step  497 / 1000 | loss 2.6841\n",
      "step  498 / 1000 | loss 2.6887\n",
      "step  499 / 1000 | loss 2.8197\n",
      "step  500 / 1000 | loss 2.3489\n",
      "step  501 / 1000 | loss 2.7506\n",
      "step  502 / 1000 | loss 2.8706\n",
      "step  503 / 1000 | loss 3.0702\n",
      "step  504 / 1000 | loss 2.1598\n",
      "step  505 / 1000 | loss 1.8368\n",
      "step  506 / 1000 | loss 2.5982\n",
      "step  507 / 1000 | loss 3.3441\n",
      "step  508 / 1000 | loss 2.1785\n",
      "step  509 / 1000 | loss 2.6984\n",
      "step  510 / 1000 | loss 2.1680\n",
      "step  511 / 1000 | loss 2.4203\n",
      "step  512 / 1000 | loss 2.3939\n",
      "step  513 / 1000 | loss 2.3744\n",
      "step  514 / 1000 | loss 2.6786\n",
      "step  515 / 1000 | loss 2.3058\n",
      "step  516 / 1000 | loss 2.5802\n",
      "step  517 / 1000 | loss 3.4715\n",
      "step  518 / 1000 | loss 2.1936\n",
      "step  519 / 1000 | loss 2.4792\n",
      "step  520 / 1000 | loss 2.7518\n",
      "step  521 / 1000 | loss 2.2246\n",
      "step  522 / 1000 | loss 3.1097\n",
      "step  523 / 1000 | loss 2.4913\n",
      "step  524 / 1000 | loss 3.5664\n",
      "step  525 / 1000 | loss 2.2553\n",
      "step  526 / 1000 | loss 2.3454\n",
      "step  527 / 1000 | loss 2.1800\n",
      "step  528 / 1000 | loss 2.2838\n",
      "step  529 / 1000 | loss 2.3926\n",
      "step  530 / 1000 | loss 1.8417\n",
      "step  531 / 1000 | loss 2.0292\n",
      "step  532 / 1000 | loss 2.5545\n",
      "step  533 / 1000 | loss 2.4119\n",
      "step  534 / 1000 | loss 3.5309\n",
      "step  535 / 1000 | loss 2.4591\n",
      "step  536 / 1000 | loss 2.3863\n",
      "step  537 / 1000 | loss 1.9716\n",
      "step  538 / 1000 | loss 1.8831\n",
      "step  539 / 1000 | loss 2.4429\n",
      "step  540 / 1000 | loss 3.1286\n",
      "step  541 / 1000 | loss 2.3458\n",
      "step  542 / 1000 | loss 3.0797\n",
      "step  543 / 1000 | loss 1.9761\n",
      "step  544 / 1000 | loss 2.6937\n",
      "step  545 / 1000 | loss 2.4707\n",
      "step  546 / 1000 | loss 2.1489\n",
      "step  547 / 1000 | loss 2.4639\n",
      "step  548 / 1000 | loss 2.7619\n",
      "step  549 / 1000 | loss 2.5304\n",
      "step  550 / 1000 | loss 2.5922\n",
      "step  551 / 1000 | loss 2.1277\n",
      "step  552 / 1000 | loss 2.2660\n",
      "step  553 / 1000 | loss 2.3106\n",
      "step  554 / 1000 | loss 2.4640\n",
      "step  555 / 1000 | loss 2.2764\n",
      "step  556 / 1000 | loss 1.9600\n",
      "step  557 / 1000 | loss 2.3043\n",
      "step  558 / 1000 | loss 1.8536\n",
      "step  559 / 1000 | loss 2.6038\n",
      "step  560 / 1000 | loss 2.0055\n",
      "step  561 / 1000 | loss 2.6274\n",
      "step  562 / 1000 | loss 1.9229\n",
      "step  563 / 1000 | loss 2.1397\n",
      "step  564 / 1000 | loss 2.3811\n",
      "step  565 / 1000 | loss 2.1963\n",
      "step  566 / 1000 | loss 2.4460\n",
      "step  567 / 1000 | loss 2.3891\n",
      "step  568 / 1000 | loss 2.3004\n",
      "step  569 / 1000 | loss 1.8246\n",
      "step  570 / 1000 | loss 2.6532\n",
      "step  571 / 1000 | loss 2.1795\n",
      "step  572 / 1000 | loss 3.0463\n",
      "step  573 / 1000 | loss 2.7860\n",
      "step  574 / 1000 | loss 2.0759\n",
      "step  575 / 1000 | loss 1.9570\n",
      "step  576 / 1000 | loss 2.0554\n",
      "step  577 / 1000 | loss 2.4251\n",
      "step  578 / 1000 | loss 2.4428\n",
      "step  579 / 1000 | loss 2.2614\n",
      "step  580 / 1000 | loss 2.3937\n",
      "step  581 / 1000 | loss 2.7125\n",
      "step  582 / 1000 | loss 3.4064\n",
      "step  583 / 1000 | loss 2.4724\n",
      "step  584 / 1000 | loss 2.3039\n",
      "step  585 / 1000 | loss 3.6543\n",
      "step  586 / 1000 | loss 1.8972\n",
      "step  587 / 1000 | loss 2.8590\n",
      "step  588 / 1000 | loss 2.2116\n",
      "step  589 / 1000 | loss 2.8182\n",
      "step  590 / 1000 | loss 2.7064\n",
      "step  591 / 1000 | loss 2.2167\n",
      "step  592 / 1000 | loss 2.6050\n",
      "step  593 / 1000 | loss 2.6488\n",
      "step  594 / 1000 | loss 2.0779\n",
      "step  595 / 1000 | loss 2.6539\n",
      "step  596 / 1000 | loss 1.9676\n",
      "step  597 / 1000 | loss 2.4949\n",
      "step  598 / 1000 | loss 1.9959\n",
      "step  599 / 1000 | loss 3.0137\n",
      "step  600 / 1000 | loss 2.5344\n",
      "step  601 / 1000 | loss 2.1148\n",
      "step  602 / 1000 | loss 1.7372\n",
      "step  603 / 1000 | loss 2.5949\n",
      "step  604 / 1000 | loss 2.4637\n",
      "step  605 / 1000 | loss 2.3960\n",
      "step  606 / 1000 | loss 1.9197\n",
      "step  607 / 1000 | loss 2.1931\n",
      "step  608 / 1000 | loss 2.7961\n",
      "step  609 / 1000 | loss 2.5193\n",
      "step  610 / 1000 | loss 1.9355\n",
      "step  611 / 1000 | loss 2.5784\n",
      "step  612 / 1000 | loss 1.8283\n",
      "step  613 / 1000 | loss 2.6203\n",
      "step  614 / 1000 | loss 2.6816\n",
      "step  615 / 1000 | loss 2.5913\n",
      "step  616 / 1000 | loss 2.8780\n",
      "step  617 / 1000 | loss 2.2517\n",
      "step  618 / 1000 | loss 2.1593\n",
      "step  619 / 1000 | loss 2.2584\n",
      "step  620 / 1000 | loss 1.9095\n",
      "step  621 / 1000 | loss 2.2301\n",
      "step  622 / 1000 | loss 2.7104\n",
      "step  623 / 1000 | loss 1.8851\n",
      "step  624 / 1000 | loss 3.1888\n",
      "step  625 / 1000 | loss 2.9876\n",
      "step  626 / 1000 | loss 2.8590\n",
      "step  627 / 1000 | loss 2.5034\n",
      "step  628 / 1000 | loss 2.4810\n",
      "step  629 / 1000 | loss 1.9359\n",
      "step  630 / 1000 | loss 2.5274\n",
      "step  631 / 1000 | loss 2.0453\n",
      "step  632 / 1000 | loss 2.4171\n",
      "step  633 / 1000 | loss 1.7924\n",
      "step  634 / 1000 | loss 2.4539\n",
      "step  635 / 1000 | loss 2.1459\n",
      "step  636 / 1000 | loss 2.5102\n",
      "step  637 / 1000 | loss 2.9568\n",
      "step  638 / 1000 | loss 2.8252\n",
      "step  639 / 1000 | loss 2.7059\n",
      "step  640 / 1000 | loss 1.9831\n",
      "step  641 / 1000 | loss 2.1923\n",
      "step  642 / 1000 | loss 2.2089\n",
      "step  643 / 1000 | loss 2.2929\n",
      "step  644 / 1000 | loss 2.0378\n",
      "step  645 / 1000 | loss 2.1292\n",
      "step  646 / 1000 | loss 2.1907\n",
      "step  647 / 1000 | loss 2.9583\n",
      "step  648 / 1000 | loss 2.2373\n",
      "step  649 / 1000 | loss 2.0723\n",
      "step  650 / 1000 | loss 2.7845\n",
      "step  651 / 1000 | loss 2.1684\n",
      "step  652 / 1000 | loss 2.4774\n",
      "step  653 / 1000 | loss 2.1871\n",
      "step  654 / 1000 | loss 2.7678\n",
      "step  655 / 1000 | loss 2.3893\n",
      "step  656 / 1000 | loss 2.1999\n",
      "step  657 / 1000 | loss 2.0858\n",
      "step  658 / 1000 | loss 2.7224\n",
      "step  659 / 1000 | loss 2.3074\n",
      "step  660 / 1000 | loss 2.3223\n",
      "step  661 / 1000 | loss 2.8183\n",
      "step  662 / 1000 | loss 3.3044\n",
      "step  663 / 1000 | loss 2.4661\n",
      "step  664 / 1000 | loss 2.0590\n",
      "step  665 / 1000 | loss 2.6969\n",
      "step  666 / 1000 | loss 2.2087\n",
      "step  667 / 1000 | loss 2.6428\n",
      "step  668 / 1000 | loss 1.9129\n",
      "step  669 / 1000 | loss 2.0428\n",
      "step  670 / 1000 | loss 2.0376\n",
      "step  671 / 1000 | loss 2.2595\n",
      "step  672 / 1000 | loss 2.4606\n",
      "step  673 / 1000 | loss 3.4269\n",
      "step  674 / 1000 | loss 2.4542\n",
      "step  675 / 1000 | loss 2.2536\n",
      "step  676 / 1000 | loss 2.2814\n",
      "step  677 / 1000 | loss 2.5887\n",
      "step  678 / 1000 | loss 2.3312\n",
      "step  679 / 1000 | loss 2.0076\n",
      "step  680 / 1000 | loss 2.2020\n",
      "step  681 / 1000 | loss 2.2884\n",
      "step  682 / 1000 | loss 2.3280\n",
      "step  683 / 1000 | loss 2.3752\n",
      "step  684 / 1000 | loss 3.1763\n",
      "step  685 / 1000 | loss 2.1159\n",
      "step  686 / 1000 | loss 2.2588\n",
      "step  687 / 1000 | loss 2.1926\n",
      "step  688 / 1000 | loss 2.3275\n",
      "step  689 / 1000 | loss 2.1344\n",
      "step  690 / 1000 | loss 3.2757\n",
      "step  691 / 1000 | loss 2.6778\n",
      "step  692 / 1000 | loss 2.3582\n",
      "step  693 / 1000 | loss 1.5838\n",
      "step  694 / 1000 | loss 2.6420\n",
      "step  695 / 1000 | loss 2.6149\n",
      "step  696 / 1000 | loss 2.1928\n",
      "step  697 / 1000 | loss 2.3794\n",
      "step  698 / 1000 | loss 2.2279\n",
      "step  699 / 1000 | loss 1.7627\n",
      "step  700 / 1000 | loss 2.3762\n",
      "step  701 / 1000 | loss 2.0529\n",
      "step  702 / 1000 | loss 1.3783\n",
      "step  703 / 1000 | loss 2.1742\n",
      "step  704 / 1000 | loss 2.3280\n",
      "step  705 / 1000 | loss 2.2085\n",
      "step  706 / 1000 | loss 2.1648\n",
      "step  707 / 1000 | loss 2.7281\n",
      "step  708 / 1000 | loss 2.2170\n",
      "step  709 / 1000 | loss 3.1714\n",
      "step  710 / 1000 | loss 2.6721\n",
      "step  711 / 1000 | loss 2.1098\n",
      "step  712 / 1000 | loss 1.9283\n",
      "step  713 / 1000 | loss 2.1694\n",
      "step  714 / 1000 | loss 2.7704\n",
      "step  715 / 1000 | loss 3.0292\n",
      "step  716 / 1000 | loss 2.8692\n",
      "step  717 / 1000 | loss 2.5061\n",
      "step  718 / 1000 | loss 2.1842\n",
      "step  719 / 1000 | loss 1.8895\n",
      "step  720 / 1000 | loss 2.1679\n",
      "step  721 / 1000 | loss 2.0281\n",
      "step  722 / 1000 | loss 3.0106\n",
      "step  723 / 1000 | loss 2.3867\n",
      "step  724 / 1000 | loss 2.3067\n",
      "step  725 / 1000 | loss 3.0596\n",
      "step  726 / 1000 | loss 1.5810\n",
      "step  727 / 1000 | loss 2.4289\n",
      "step  728 / 1000 | loss 2.1859\n",
      "step  729 / 1000 | loss 2.3616\n",
      "step  730 / 1000 | loss 2.1707\n",
      "step  731 / 1000 | loss 2.4930\n",
      "step  732 / 1000 | loss 3.0315\n",
      "step  733 / 1000 | loss 2.6223\n",
      "step  734 / 1000 | loss 2.2961\n",
      "step  735 / 1000 | loss 1.8732\n",
      "step  736 / 1000 | loss 2.4116\n",
      "step  737 / 1000 | loss 2.2715\n",
      "step  738 / 1000 | loss 2.5059\n",
      "step  739 / 1000 | loss 2.2547\n",
      "step  740 / 1000 | loss 2.5099\n",
      "step  741 / 1000 | loss 2.5576\n",
      "step  742 / 1000 | loss 2.6107\n",
      "step  743 / 1000 | loss 2.0935\n",
      "step  744 / 1000 | loss 2.0477\n",
      "step  745 / 1000 | loss 2.5853\n",
      "step  746 / 1000 | loss 2.4907\n",
      "step  747 / 1000 | loss 2.4692\n",
      "step  748 / 1000 | loss 1.8996\n",
      "step  749 / 1000 | loss 2.9825\n",
      "step  750 / 1000 | loss 2.8359\n",
      "step  751 / 1000 | loss 2.5020\n",
      "step  752 / 1000 | loss 2.4697\n",
      "step  753 / 1000 | loss 1.6417\n",
      "step  754 / 1000 | loss 2.7585\n",
      "step  755 / 1000 | loss 2.5317\n",
      "step  756 / 1000 | loss 2.4623\n",
      "step  757 / 1000 | loss 2.1052\n",
      "step  758 / 1000 | loss 2.0988\n",
      "step  759 / 1000 | loss 2.5586\n",
      "step  760 / 1000 | loss 2.7581\n",
      "step  761 / 1000 | loss 2.6869\n",
      "step  762 / 1000 | loss 1.9811\n",
      "step  763 / 1000 | loss 2.3683\n",
      "step  764 / 1000 | loss 2.8030\n",
      "step  765 / 1000 | loss 2.3591\n",
      "step  766 / 1000 | loss 2.5555\n",
      "step  767 / 1000 | loss 2.1909\n",
      "step  768 / 1000 | loss 3.7710\n",
      "step  769 / 1000 | loss 2.0050\n",
      "step  770 / 1000 | loss 1.9931\n",
      "step  771 / 1000 | loss 2.1312\n",
      "step  772 / 1000 | loss 2.6808\n",
      "step  773 / 1000 | loss 2.2296\n",
      "step  774 / 1000 | loss 2.0894\n",
      "step  775 / 1000 | loss 2.7163\n",
      "step  776 / 1000 | loss 1.7494\n",
      "step  777 / 1000 | loss 2.0326\n",
      "step  778 / 1000 | loss 1.9350\n",
      "step  779 / 1000 | loss 2.7923\n",
      "step  780 / 1000 | loss 2.2119\n",
      "step  781 / 1000 | loss 2.8312\n",
      "step  782 / 1000 | loss 2.6453\n",
      "step  783 / 1000 | loss 2.7052\n",
      "step  784 / 1000 | loss 3.7975\n",
      "step  785 / 1000 | loss 2.9987\n",
      "step  786 / 1000 | loss 2.4779\n",
      "step  787 / 1000 | loss 2.0027\n",
      "step  788 / 1000 | loss 2.1539\n",
      "step  789 / 1000 | loss 2.3223\n",
      "step  790 / 1000 | loss 2.3931\n",
      "step  791 / 1000 | loss 1.9229\n",
      "step  792 / 1000 | loss 2.4534\n",
      "step  793 / 1000 | loss 1.9308\n",
      "step  794 / 1000 | loss 2.7655\n",
      "step  795 / 1000 | loss 1.7388\n",
      "step  796 / 1000 | loss 3.0472\n",
      "step  797 / 1000 | loss 2.7037\n",
      "step  798 / 1000 | loss 2.0475\n",
      "step  799 / 1000 | loss 2.2556\n",
      "step  800 / 1000 | loss 2.0617\n",
      "step  801 / 1000 | loss 1.9115\n",
      "step  802 / 1000 | loss 2.6545\n",
      "step  803 / 1000 | loss 1.7789\n",
      "step  804 / 1000 | loss 2.2043\n",
      "step  805 / 1000 | loss 2.8895\n",
      "step  806 / 1000 | loss 2.7634\n",
      "step  807 / 1000 | loss 2.3037\n",
      "step  808 / 1000 | loss 2.6828\n",
      "step  809 / 1000 | loss 2.4518\n",
      "step  810 / 1000 | loss 2.0304\n",
      "step  811 / 1000 | loss 3.4453\n",
      "step  812 / 1000 | loss 2.6130\n",
      "step  813 / 1000 | loss 2.2491\n",
      "step  814 / 1000 | loss 2.4675\n",
      "step  815 / 1000 | loss 1.9479\n",
      "step  816 / 1000 | loss 2.7025\n",
      "step  817 / 1000 | loss 2.0691\n",
      "step  818 / 1000 | loss 2.1546\n",
      "step  819 / 1000 | loss 2.6003\n",
      "step  820 / 1000 | loss 2.7238\n",
      "step  821 / 1000 | loss 2.0403\n",
      "step  822 / 1000 | loss 2.0342\n",
      "step  823 / 1000 | loss 3.0437\n",
      "step  824 / 1000 | loss 1.8764\n",
      "step  825 / 1000 | loss 2.4629\n",
      "step  826 / 1000 | loss 2.5016\n",
      "step  827 / 1000 | loss 2.9541\n",
      "step  828 / 1000 | loss 2.8198\n",
      "step  829 / 1000 | loss 3.4210\n",
      "step  830 / 1000 | loss 2.2760\n",
      "step  831 / 1000 | loss 2.0028\n",
      "step  832 / 1000 | loss 2.3626\n",
      "step  833 / 1000 | loss 2.6164\n",
      "step  834 / 1000 | loss 2.1973\n",
      "step  835 / 1000 | loss 2.3554\n",
      "step  836 / 1000 | loss 3.1209\n",
      "step  837 / 1000 | loss 2.0635\n",
      "step  838 / 1000 | loss 2.2788\n",
      "step  839 / 1000 | loss 2.4414\n",
      "step  840 / 1000 | loss 2.5356\n",
      "step  841 / 1000 | loss 2.6720\n",
      "step  842 / 1000 | loss 2.4237\n",
      "step  843 / 1000 | loss 2.8558\n",
      "step  844 / 1000 | loss 2.4761\n",
      "step  845 / 1000 | loss 2.2398\n",
      "step  846 / 1000 | loss 2.3471\n",
      "step  847 / 1000 | loss 3.7339\n",
      "step  848 / 1000 | loss 2.7431\n",
      "step  849 / 1000 | loss 2.7992\n",
      "step  850 / 1000 | loss 1.9087\n",
      "step  851 / 1000 | loss 2.5279\n",
      "step  852 / 1000 | loss 2.3873\n",
      "step  853 / 1000 | loss 2.4366\n",
      "step  854 / 1000 | loss 2.4126\n",
      "step  855 / 1000 | loss 2.0480\n",
      "step  856 / 1000 | loss 1.6719\n",
      "step  857 / 1000 | loss 2.5517\n",
      "step  858 / 1000 | loss 1.8625\n",
      "step  859 / 1000 | loss 1.9764\n",
      "step  860 / 1000 | loss 2.5352\n",
      "step  861 / 1000 | loss 2.1741\n",
      "step  862 / 1000 | loss 2.3557\n",
      "step  863 / 1000 | loss 2.4006\n",
      "step  864 / 1000 | loss 2.1570\n",
      "step  865 / 1000 | loss 2.8819\n",
      "step  866 / 1000 | loss 1.6597\n",
      "step  867 / 1000 | loss 2.6132\n",
      "step  868 / 1000 | loss 2.7801\n",
      "step  869 / 1000 | loss 2.5996\n",
      "step  870 / 1000 | loss 2.1378\n",
      "step  871 / 1000 | loss 2.8827\n",
      "step  872 / 1000 | loss 2.4199\n",
      "step  873 / 1000 | loss 2.3633\n",
      "step  874 / 1000 | loss 2.1089\n",
      "step  875 / 1000 | loss 2.9548\n",
      "step  876 / 1000 | loss 2.7262\n",
      "step  877 / 1000 | loss 2.5398\n",
      "step  878 / 1000 | loss 1.9931\n",
      "step  879 / 1000 | loss 2.1492\n",
      "step  880 / 1000 | loss 2.7779\n",
      "step  881 / 1000 | loss 1.8344\n",
      "step  882 / 1000 | loss 2.5124\n",
      "step  883 / 1000 | loss 2.1613\n",
      "step  884 / 1000 | loss 2.8918\n",
      "step  885 / 1000 | loss 2.7259\n",
      "step  886 / 1000 | loss 3.0782\n",
      "step  887 / 1000 | loss 2.4595\n",
      "step  888 / 1000 | loss 2.5622\n",
      "step  889 / 1000 | loss 2.6963\n",
      "step  890 / 1000 | loss 2.4399\n",
      "step  891 / 1000 | loss 2.0037\n",
      "step  892 / 1000 | loss 1.7691\n",
      "step  893 / 1000 | loss 2.4010\n",
      "step  894 / 1000 | loss 2.4448\n",
      "step  895 / 1000 | loss 1.9977\n",
      "step  896 / 1000 | loss 1.9576\n",
      "step  897 / 1000 | loss 1.8725\n",
      "step  898 / 1000 | loss 2.1446\n",
      "step  899 / 1000 | loss 2.0693\n",
      "step  900 / 1000 | loss 2.5108\n",
      "step  901 / 1000 | loss 2.3291\n",
      "step  902 / 1000 | loss 2.6489\n",
      "step  903 / 1000 | loss 2.2840\n",
      "step  904 / 1000 | loss 2.3829\n",
      "step  905 / 1000 | loss 2.1373\n",
      "step  906 / 1000 | loss 2.3217\n",
      "step  907 / 1000 | loss 2.3272\n",
      "step  908 / 1000 | loss 2.2167\n",
      "step  909 / 1000 | loss 2.2839\n",
      "step  910 / 1000 | loss 2.2612\n",
      "step  911 / 1000 | loss 2.3539\n",
      "step  912 / 1000 | loss 2.2818\n",
      "step  913 / 1000 | loss 2.4401\n",
      "step  914 / 1000 | loss 2.1941\n",
      "step  915 / 1000 | loss 1.8256\n",
      "step  916 / 1000 | loss 2.7303\n",
      "step  917 / 1000 | loss 2.1595\n",
      "step  918 / 1000 | loss 2.2792\n",
      "step  919 / 1000 | loss 2.0663\n",
      "step  920 / 1000 | loss 1.7833\n",
      "step  921 / 1000 | loss 2.9049\n",
      "step  922 / 1000 | loss 2.6700\n",
      "step  923 / 1000 | loss 2.4037\n",
      "step  924 / 1000 | loss 2.6787\n",
      "step  925 / 1000 | loss 2.0704\n",
      "step  926 / 1000 | loss 2.7046\n",
      "step  927 / 1000 | loss 2.3844\n",
      "step  928 / 1000 | loss 2.1604\n",
      "step  929 / 1000 | loss 2.3151\n",
      "step  930 / 1000 | loss 2.0690\n",
      "step  931 / 1000 | loss 1.7586\n",
      "step  932 / 1000 | loss 1.8078\n",
      "step  933 / 1000 | loss 2.5376\n",
      "step  934 / 1000 | loss 2.8071\n",
      "step  935 / 1000 | loss 2.1101\n",
      "step  936 / 1000 | loss 2.5462\n",
      "step  937 / 1000 | loss 3.2008\n",
      "step  938 / 1000 | loss 1.8840\n",
      "step  939 / 1000 | loss 1.9188\n",
      "step  940 / 1000 | loss 2.0666\n",
      "step  941 / 1000 | loss 2.3070\n",
      "step  942 / 1000 | loss 3.1447\n",
      "step  943 / 1000 | loss 2.1992\n",
      "step  944 / 1000 | loss 2.6754\n",
      "step  945 / 1000 | loss 2.2604\n",
      "step  946 / 1000 | loss 2.5819\n",
      "step  947 / 1000 | loss 2.2409\n",
      "step  948 / 1000 | loss 2.1661\n",
      "step  949 / 1000 | loss 2.2039\n",
      "step  950 / 1000 | loss 1.9313\n",
      "step  951 / 1000 | loss 2.2498\n",
      "step  952 / 1000 | loss 2.3982\n",
      "step  953 / 1000 | loss 2.0357\n",
      "step  954 / 1000 | loss 2.2171\n",
      "step  955 / 1000 | loss 2.1470\n",
      "step  956 / 1000 | loss 1.7951\n",
      "step  957 / 1000 | loss 1.9757\n",
      "step  958 / 1000 | loss 2.5849\n",
      "step  959 / 1000 | loss 2.0927\n",
      "step  960 / 1000 | loss 1.9038\n",
      "step  961 / 1000 | loss 2.2233\n",
      "step  962 / 1000 | loss 2.1794\n",
      "step  963 / 1000 | loss 1.8836\n",
      "step  964 / 1000 | loss 2.6015\n",
      "step  965 / 1000 | loss 1.8587\n",
      "step  966 / 1000 | loss 2.2744\n",
      "step  967 / 1000 | loss 2.3596\n",
      "step  968 / 1000 | loss 2.0168\n",
      "step  969 / 1000 | loss 2.5149\n",
      "step  970 / 1000 | loss 2.0698\n",
      "step  971 / 1000 | loss 2.2743\n",
      "step  972 / 1000 | loss 2.8700\n",
      "step  973 / 1000 | loss 3.2256\n",
      "step  974 / 1000 | loss 2.7623\n",
      "step  975 / 1000 | loss 2.1574\n",
      "step  976 / 1000 | loss 2.3514\n",
      "step  977 / 1000 | loss 2.6426\n",
      "step  978 / 1000 | loss 2.4010\n",
      "step  979 / 1000 | loss 2.4923\n",
      "step  980 / 1000 | loss 1.8694\n",
      "step  981 / 1000 | loss 1.9656\n",
      "step  982 / 1000 | loss 2.7231\n",
      "step  983 / 1000 | loss 4.3990\n",
      "step  984 / 1000 | loss 2.0685\n",
      "step  985 / 1000 | loss 2.9842\n",
      "step  986 / 1000 | loss 2.2010\n",
      "step  987 / 1000 | loss 1.7576\n",
      "step  988 / 1000 | loss 2.2139\n",
      "step  989 / 1000 | loss 1.6517\n",
      "step  990 / 1000 | loss 2.5471\n",
      "step  991 / 1000 | loss 2.2228\n",
      "step  992 / 1000 | loss 1.9374\n",
      "step  993 / 1000 | loss 2.1337\n",
      "step  994 / 1000 | loss 2.1799\n",
      "step  995 / 1000 | loss 2.0999\n",
      "step  996 / 1000 | loss 2.5326\n",
      "step  997 / 1000 | loss 2.8580\n",
      "step  998 / 1000 | loss 2.8348\n",
      "step  999 / 1000 | loss 1.8863\n",
      "step 1000 / 1000 | loss 3.0042\n"
     ]
    }
   ],
   "source": [
    "learning_rate,beta1,beta2,eps_adam=0.01,0.85,0.99,1e-8\n",
    "m=[0.0] * len(parms)\n",
    "v=[0.0] * len(parms)\n",
    "\n",
    "nums_steps=1000\n",
    "for step in range(nums_steps):\n",
    "    doc=docs[step%len(docs)]\n",
    "    tokens=[BOS]+[uchars.index(ch) for ch in doc] +[BOS]\n",
    "    n= min(block_size,len(tokens)-1)\n",
    "    keys, values = [[]for _ in range(n_layer)],[[] for _ in range(n_layer)]\n",
    "    losses=[]\n",
    "    for pos_id in range(n):\n",
    "        token_id, target_id = tokens[pos_id], tokens[pos_id +1]\n",
    "        logits=gpt(token_id,pos_id,keys,values)\n",
    "        probs= softmax(logits)\n",
    "        loss_t= -probs[target_id].log()\n",
    "        losses.append(loss_t)\n",
    "    loss=(1/n)*sum(losses)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    lr_t=learning_rate*(1-step/nums_steps) # lr decay\n",
    "\n",
    "    for i, p in enumerate(parms):\n",
    "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
    "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
    "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
    "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
    "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
    "        p.grad = 0\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"step {step+1:4d} / {nums_steps:4d} | loss {loss.data:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c96dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- inference (new, hallucinated names) ---\n",
      "sample  1: kindah\n",
      "sample  2: aemis\n",
      "sample  3: kaaia\n",
      "sample  4: jalil\n",
      "sample  5: kelin\n",
      "sample  6: jasha\n",
      "sample  7: li\n",
      "sample  8: areeli\n",
      "sample  9: kerona\n",
      "sample 10: cori\n",
      "sample 11: jadan\n",
      "sample 12: naele\n",
      "sample 13: reali\n",
      "sample 14: leela\n",
      "sample 15: aveli\n",
      "sample 16: jaril\n",
      "sample 17: aranon\n",
      "sample 18: shalya\n",
      "sample 19: aacth\n",
      "sample 20: aronai\n"
     ]
    }
   ],
   "source": [
    "temperature=0.5\n",
    "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
    "for sample_idx in range(20):\n",
    "    keys, values = [[]for _ in range(n_layer)],[[] for _ in range(n_layer)]\n",
    "    token_id=BOS\n",
    "    sample=[]\n",
    "    for pos_id in range(block_size):\n",
    "        logits= gpt(token_id,pos_id,keys,values)\n",
    "        prbs=softmax([l/temperature for l in logits])\n",
    "        token_id= random.choices(range(vocab_size),weights=[p.data for p in prbs])[0]\n",
    "        # token_id = max(\n",
    "        #     range(vocab_size),\n",
    "        #     key=lambda i: probs[i].data\n",
    "        # )\n",
    "        if token_id==BOS:\n",
    "            break\n",
    "        sample.append(uchars[token_id])\n",
    "    \n",
    "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75881ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
